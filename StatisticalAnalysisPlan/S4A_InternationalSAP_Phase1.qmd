---
title: "Start 4 All Phase 1 - Statistical Analysis Plan"
author: "Marc Henrion"
subtitle: "v1.2 (International)"
date: "`r format(Sys.time(), '%d %B %Y')`"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 3
    fig-width: 8
    fig-height: 6
    code-fold: true
    page-layout: full
#prefer-html: true
execute:
  echo: true
  warning: false
  error: false
  message: false
bibliography: S4A_SAP.json
number-sections: true
---

```{r setup}
#| include: false

library(tidyverse)
library(kableExtra)
library(knitr)
library(grid)
library(gridExtra)
library(bootComb)
library(rjags)
library(coda)
library(MCMCvis)
library(rms)

options(knitr.kable.NA = '.')
```

<!--

# TO DO LIST

* Implement final algorithm list.

* Explore use of Decision Curve Analysis (DCA; see Tom's email from 9 April 2024 [https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(24)00029-9/fulltext](https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(24)00029-9/fulltext) and [https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(24)00061-5/fulltext](https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(24)00061-5/fulltext)). DCA was introduced in 2006 [https://doi.org/10.1177%2F0272989X06295361](https://doi.org/10.1177%2F0272989X06295361) and is recommended by TRIPOD for evaluating prediction algorithms [https://doi.org/10.1016%2Fj.eururo.2018.08.038](https://doi.org/10.1016%2Fj.eururo.2018.08.038) and [https://doi.org/10.1016/j.eururo.2014.11.025](https://doi.org/10.1016/j.eururo.2014.11.025).

-->

# List of abbreviations

@tbl-abbr lists all abbreviations used throughout this document.

```{r}
#| echo: false
#| label: tbl-abbr
#| tbl-cap: "List of abbreviations"

dfAbbr<-data.frame(
  abbr=c("ACF"),
  details=c("Active case finding; the process of actively identifying people with TB. ACF aims to identify people with TB either in those who do not recognize that they have symptoms, or those who do recognize symptoms but for whatever reason do not, or cannot, access services at health-care facilities.")
)
dfAbbr<-rbind(dfAbbr,c("CAD","Computer aided diagnostic"))
dfAbbr<-rbind(dfAbbr,c("CRP","C reactive protein; a blood inflammatory marker measured using point of care devices or laboratory assays, which is elevated in the presence of infections, inflammatory or neoplastic (cancer) conditions, including TB. WHO recommends CRP as a screening tool for TB among people living with HIV and research studies indicate CRP  could also have a use in TB screening among people not infected with HIV."))
dfAbbr<-rbind(dfAbbr,c("CXR","Chest x-ray"))
dfAbbr<-rbind(dfAbbr,c("GA","Gastric aspirate"))
dfAbbr<-rbind(dfAbbr,c("ICF","Intensified case finding at health facilities; an activity, recommended by the WHO, intended to detect possible TB cases among people attending health facilities for reasons not associated with TB (e.g. general clinics or maternity wards)."))
dfAbbr<-rbind(dfAbbr,c("IDP","Internally displaced people"))
dfAbbr<-rbind(dfAbbr,c("LAM","Lipoarabinomannan"))
dfAbbr<-rbind(dfAbbr,c("NPA","Nasopharyngeal aspiration"))
dfAbbr<-rbind(dfAbbr,c("NPV","Negative predictive value"))
dfAbbr<-rbind(dfAbbr,c("PCF","Passive case finding"))
dfAbbr<-rbind(dfAbbr,c("PLWH","People / person living with HIV"))
dfAbbr<-rbind(dfAbbr,c("POC","Point-of-care"))
dfAbbr<-rbind(dfAbbr,c("PPV","Positive predictive value"))
dfAbbr<-rbind(dfAbbr,c("RDT","Rapid diagnostic test"))
dfAbbr<-rbind(dfAbbr,c("SAP","Statistical analysis plan"))
dfAbbr<-rbind(dfAbbr,c("TB","tuberculosis"))
dfAbbr<-rbind(dfAbbr,c("MCMC","Markov Chain Monte Carlo"))
dfAbbr<-rbind(dfAbbr,c("SSM","Sputum smear microscopy"))
dfAbbr<-rbind(dfAbbr,c("DYT","Diagnostic yield per test"))
#dfAbbr<-rbind(dfAbbr,c("",""))

dfAbbr<-dfAbbr[order(dfAbbr$abbr),]

dfAbbr %>%
  kable(row.names=F,col.names=c("Abbreviation","Explanation")) %>%
  kable_styling(full_width=FALSE)
```


# Protocol version

This Statistical Analysis Plan (SAP) v1.2 is based on the Start 4 All International Protocol v1.6, dated 22 May 2023.

## Version history

```{r}
#| echo: false
#| label: tbl-versionHistory
#| tbl-cap: "Version history of the SAP since v1.0."

dfHist<-data.frame(
  Version=c("1.2","1.1","1.0"),
  Date=c("2024-05-27","2025-05-26","2025-05-25"),
  Author=c("Marc Henrion","Marc Henrion","Marc Henrion"),
  Edits=c("Clarified agorithms with pooled Xpert Ultra testing and updated JAGS files correspondingly","Mock-ups added; full JAGS model files added; details of MCMC run parameters; added JAGS version number; slightly revised secondary analyses","First full version of the SAP (while development versions 0.1 to 0.14 exist, no version history record was kept prior to v1.0)")
)

dfHist %>%
  kable() %>%
  kable_styling(full_width = FALSE)
```


# Protocol summary

Start 4 All is a large, UNITAID funded 4-year tuberculosis (TB) study programme, running in 7 countries and consisting of 2 phases:

* Phase 1 (18 months) - cross-sectional surveys to generate data on diagnostic assay performances in different countries and different populations and identifying optimal diagnostic test combinations (for screening purposes).

* Phase 2 (30 months) - a large, multi-country implementation study to evaluate the chosen optiomal combinations in the different settings.

This SAP focuses on Phase 1, with a separate SAP being written for the trial in Phase 2. There will also be a separate analysis plan for the economic evaluation of the diagnostic test combinations.

## Study design

Phase 1 consists of multiple cross-sectional surveys in 7 countries and in different general and marginalised populations within those countries.

```{r}
#| echo: false
#| label: tbl-countries
#| tbl-cap: "Summary of study populations, countries, and procedures."

dfCountries<-data.frame(
  Population=c("Primary health care attenders","District / secondary hospital attenders","Key / marginalised populations (informal settlements, IPDs, refugees, rural poor, pastoralists)","Children"),
  CurrentApproach=c("Sympton screen; sputum smear microscopy; sputum transport for Xpert","Symptom screen; sputum Xper testing (where available); 1st gen LAM for HIV positive admissions (where avilable)","Symptom screen; mobile chest X-ray (predominantly human interpretation); sputum transport for smear/Xpert/culture","Symptom screen; Xpert testing (stool, NPA, GA, where available); X-ray (human interpretation); IGRA (Vietnam)"),
  LocalLaboratory=c("None / basic","Medium / some with Xpert","None","Medium / some with Xpert"),
  Country=c("Bangladesh, Brazil, Cameroon, Kenya, Malawi, Nigeria, Vietnam","Bangladesh, Cameroon, Kenya, Malawi, Nigeria, Vietnam","Bangladesh, Cameroon, Nigeria","Cameroon, Vietnam"),
  XrayAvailability=c("No","Yes","Limited / mobile trucks","Yes (if in hospital)"),
  CaseFindingApproach=c("PCF / ICF","PCF / ICF","PCF / ICF / ACF","PCF / ICF / ACF"),
  PercentWithTBAmongTested=c("5-15%","10-20%","0.5-10%","8-12%"),
  AdditionTBDiagnostics=c("CAD CXR, POC CRP, CRP RDT, LAM, pooling Ultra","CAD CXR, POC CRP, CRP RDT, LAM, pooling Ultra","CAD CXR, POC CRP, CRP RDT, LAM, Pooling Ultra","CAD CXR, POC CRP, CRP RDT, LAM, pooling Ultra")
)

dfCountries %>% 
  kable(row.names=FALSE,col.names=c("Population","Currently implemented TB screening / diagnosis approach","Local laboratory","Country","X-ray availability","Case finding approach","% with TB among those testes","Additional diagnostic tests and test combinations evaluated as part of Start 4 All"),format = "html")
# %>% kableExtra::footnote(general_title="",general="NPA = nasopharyngeal aspiration, GA = gastric aspirate, CAD = computer assistant diagnostics, POC CRP = point-of-care CRP (quantitative), CRP RDT = CRP rapid diagnostic test (semiquantitative).")
```

@tbl-countries summarises the different surveys. This study design was selected because (together with diagnostic clinical trials) surveys are considered the best design for the evaluation of diagnostics. We will evaluate the performance of HIV testing, CRP, computer assisted digital X-Rays (CAD CXR), testing multiple sputum samples by pooling them together and testing the pool with a single Xpert Ultra cartridge and urine lipoarabinomannan (LAM) as screening tests for TB. Although the performance of these tests has been described among individuals with presumptive TB, there are now updated prototypes (e.g. LAM) or software (e.g. CAD CXR) for some platforms, there is limited information on their performance among key populations (e.g. nomads/internally displaced people) and hardly any information on their performance in primary health care settings.

A composite microbiological reference standard will be used to describe the sensitivity, specificity and positive and negative predictive value of different assays and their combinations. The composite standard will classify participants as ‘microbiologically confirmed’, if their sputum culture or Xpert Ultra (as a single test) are positive or as ‘unlikely TB’ if they have negative culture AND negative Xpert Ultra results.

@tbl-diagnostics summarises the screening assays evaluated in this study.

```{r}
#| echo: false
#| label: tbl-diagnostics
#| tbl-cap: "Summary of screening and diagnostic assays evaluated in this study."

dfDiags<-data.frame(
  AssayType=c("HIV testing","Point-of-care C-Reactive Protein (quantitative)","Point-of-care C-Reactive Protein (semi-quantitative)","Computer-aided detection from digital chest X-ray","Urine Lateral Flow Test","Molecular diagnostic","Molecular diagnostic"),
  Assay=c("HIV","POC CRP","CRP RDT","Digital chest X-ray + Qure.ai","FujiLAM","Xpert Ultra","Pooled Xpert Ultra")
)

dfDiags %>% 
  kable(row.names=FALSE,col.names=c("Assay type","Assay"),format = "html")
```


## Study Endpoints

### Primary endpoint

Generation of robust estimates of the diagnostic accuracy and performance of TB diagnostic tests and test combinations in primary healthcare settings and key and vulnerable populations.

###	Secondary study endpoints

* Estimates of the predicted performance of TB diagnostic test combinations and how they perform in specific populations.

* Selection of the optimal TB diagnostic test combinations for implementation and scale-up in primary healthcare settings and key and vulnerable populations in terms of their cost-efficiency, feasibility, and modelled accuracy. 


## Sample size

Diagnostic test combinations for the Phase 1 cross-sectional studies are tailored by country and key populations. We envisage conducting one cross sectional study among pastoralists, one study among refugees and IDPs, one among people living in informal settlements, one among the rural poor, and 6 studies among populations in clinical facilities (total of 18 cross sectional studies across 6 countries). Each study will comprise about 600 participants, including a minimum of 100 bacteriologically confirmed participants in most populations. The key populations and number of studies for Phase 1 studies are shown below.

The sample sizes were chosen to comply with the sample size criterion required for WHO guideline development and to achieve an acceptable level of precision for the estimates of the performance of the diagnostic tests. The sample sizes are also suitable for estimating costs and cost-efficiency of the TB diagnostic combinations.

tbl-sampSizeCalc gives the number of TB cases / number of subjects at different prevalence values for TB and different values of precision for 95% CI of point estimate of number of bacteriologically confirmed TB cases identified from each sample population. Populations with low proportions (<10%) of bacteriologically confirmed TB (children, ACF in Vietnam), will use the same cross-sectional design, but the number of bacteriologically confirmed participants will be purposely enriched by inviting participants attending adjacent centers to the study centers. Once included in the study, participants will undergo all tests in the diagnostic combination.

The calculations in tbl-sampSizeCalc have been obtained using the standard Wald normal approximation:

$$
n_{unadj}=p\cdot(1-p)\cdot\left(\frac{z_{\alpha/2}}{E}\right)^2
$$
where $p$ = target proportion (sensitivity in your case; $p=0.9$), E = target precision (or margin-of-error) and $z_{\alpha/2}$ is the (two-sided) critical value from the normal distribution for significance level $\alpha$ (in our case $\alpha=0.05$ and $z_{alpha/2}=1.96$).

```{r}
#| label: tbl-sampSizeCalc
#| tbl-cap: "Sample size calculation for adults."

sampSize_OnePropCI<-function(p_hat,E=NULL,n=NULL,alpha=0.05,method="WaldNormalApprox"){
  # p_hat = expected proportion
  # E = desired error of margin (half width of the confidence interval); if specified then n gets computed
  # n = fixed sample size; if specified then E gets computed
  # alpha = 1 - confidence level
  # method = one of "WaldNormalApprox" or "WilsonScoreApprox"
  # NOTE: exactly one of n, E needs to be specified -- they cannot both be set to NULL or both be specified
  
  z<-qnorm(1-alpha/2)
  
  if(is.null(n) & !is.null(E)){
    if(method=="WaldNormalApprox"){
      n<-p_hat*(1-p_hat)*(z/E)^2
    }else if(method=="WilsonScoreApprox"){
      a<-1
      b<-(2-p_hat*(1-p_hat)/E^2)*z^2
      c<-(4*E^2-1)*(z^4)/(4*E^2)
      n<-(-b + sqrt(b^2-4*a*c))/(2*a)
    }else{stop("The method parameter needs to be one of WaldNormalApprox or WilsonScoreApprox")}
    
    return(n)
  }else if (is.null(E) & !is.null(n)){
    if(method=="WilsonScoreApprox"){warning("Argument method is set to WilsonScoreApprox but this will be ignored and WaldNormalApprox be used instead.")}
    E<-sqrt((p_hat*(1-p_hat)*z^2)/n)
    
    return(E)
  }else{
    stop("E and n cannot both be NULL or both be specified - you need to specify one of these two and leave the other unspecified / NULL.")
  }
  
}

dfSS<-data.frame(
  tbPrevalence=c(1:5,8:10,12,15,20,27)/100,
  prec5=NA,
  prec6=NA,
  prec7=NA,
  prec8=NA,
  prec10=NA,
  prec15=NA
) %>%
  mutate(
    prec5=paste(sep=" / ",ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.05)),ceiling(c(ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.05))/tbPrevalence))),
    prec6=paste(sep=" / ",ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.06)),ceiling(c(ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.06))/tbPrevalence))),
    prec7=paste(sep=" / ",ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.07)),ceiling(c(ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.07))/tbPrevalence))),
    prec8=paste(sep=" / ",ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.08)),ceiling(c(ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.08))/tbPrevalence))),
    prec10=paste(sep=" / ",ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.1)),ceiling(c(ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.1))/tbPrevalence))),
    prec15=paste(sep=" / ",ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.15)),ceiling(c(ceiling(sampSize_OnePropCI(p_hat=0.9,E=0.15))/tbPrevalence)))
  ) %>%
  mutate(tbPrevalence=paste(sep="",100*tbPrevalence,"%"))

dfSS %>%
  kable(row.names=FALSE,col.names=c("","5%","6%","7%","8%","10%","15%")) %>%
  kable_styling(full_width=FALSE) %>%
  add_header_above(c("TB prevalence"=1,"Precision"=6)) %>%
  add_header_above(c(" "=1,"Expected number microbiologically confirmed / number required in sample"=6))
```

The resulting number $n_{unadj}$ needs to be adjusted for the TB prevalence, $p_{TB}$:

$$
n_{adj}=\frac{n_{unadj}}{p_{TB}}
$$

We further need to take into account different levels of expected attrition (due to loss-to-follow-up; unusable data or other causes):

$$
n_{final}=\frac{n_{adj}}{1-p_{attr}}
$$

where $p_{attr}$ is the expected proportion of attrition.

The studies will be analyzed individually by countries, and then as a single multi-country evaluation. The latter aims to achieve the requirements of data for WHO guidance, which requests at least 250 individuals with bacteriologically confirmed TB.

As shown in @tbl-sampSizeTableByCountry, the total sample size to be enrolled varies with the average TB prevalence, target precision and expected loss-to-follow-up (individuals not completing a diagnostic process) in the individual countries and sites. However, with the assumptions shown in @tbl-sampSizeTableByCountry, and assuming an overall proportion of 50% people living with HIV (PLWH) adult participants, we estimate a sample size of 7,200 PLWH and 7,200 HIV-negative persons will include at least 548 PLWH and 548 HIV-negative persons (total 1,096) with a microbiologically confirmed diagnosis of TB.

It is expected that countries with a high burden of HIV-associated TB (Brazil, Cameroon, Kenya, Malawi, Nigeria) will recruit an even higher ratio of PLWH to HIV-negative participants. If the recruitment of PLWH is lower than expected, countries with high burden of HIV-associated TB will enrich recruitment towards this population by recruiting in HIV care and treatment clinics and outpatient centres.

The number of adult males is expected to be higher than the number of females, as more males are affected by TB and a higher proportion of males than females attend primary and secondary clinics. The proportion of participants with TB that is obtained at the end of Phase 1 may vary from the expected proportion of participants with TB due to the inclusion of ICF and ACF in the study in settings in which they are not normally applied (i.e. ICF being conducted in PHC clinics). Additionally, some populations maybe feel more encouraged or discouraged to participate in the study, which may be reflected in the proportion of TB cases for a particular setting and the reasons behind this may be elucidated during the Realist Evaluation.

```{r}
#| label: tbl-sampSizeTableByCountry
#| tbl-cap: "Sample size for each survey."

dfSSCountry<-data.frame(
  country=c(rep("Cameroon",4),rep("Nigeria",4),rep("Kenya",2),rep("Bangladesh",3),rep("Brazil",2),rep("Vietnam",3),rep("Malawi",1),"TOTAL"),
  setting=c("PHC clinics","District hospital","Informal settlement / rural poor ACF","Children","PHC clinics","District hospital","Nomads","IDP / refugees","PHC clinics","District hospital","PHC clinics","District hospital","Informal settlements / rural poor ACF","PHC clinics Aracaju","PHC clinics Maceio","PHC clinics","Informal settlements ACF","Children","PHC clinics",NA),
  expectedTbProp=c(paste(sep="",c(9,8,5,4,10,20,8,12,9,20,5,3,5,15,27,2,1,0,6),"%"),NA),
  targetPrecision=c(paste(sep="",c(7,7,10,15,6.5,5.5,7,6,8,5.5,9,12,9,7,5.5,15,17,NA,8),"%"),NA),
  expectedAttrition=c(paste(sep="",c(20,12,15,20,18,18,12,20,12,18,14,16,14,5,10,20,20,NA,20),"%"),NA),
  sampleSize=c(c(1000,1000,800,500,1000,700,1000,1000,700,700,1000,1000,1000,500,500,1000,1500,500,1200),NA),
  expectedAdultTB=c(72,70,34,NA,82,114,70,96,55,114,43,26,43,71,121,16,12,NA,57,1096),
  adults=c(2800,rep(NA,3),3700,rep(NA,3),1400,NA,3000,rep(NA,2),1000,NA,2500,rep(NA,2),1200,14400),
  children=c(500,rep(NA,3),rep(NA,4),rep(NA,2),rep(NA,3),rep(NA,2),500,rep(NA,2),NA,1000)
)

dfSSCountry[dfSSCountry=="NA%"]<-NA

dfSSCountry %>%
  dplyr::select(!country) %>%
  kable(row.names=FALSE,col.names=c("Country & setting","Expected proportion with TB","Target precision","Expected attrition","Sample size","Expected adults with TB","Adults","Children"),format.args = list(big.mark = ",")) %>%
  kable_styling(full_width = FALSE) %>%
  pack_rows("Cameroon",1,4) %>%
  pack_rows("Nigeria",5,8) %>%
  pack_rows("Kenya",9,10) %>%
  pack_rows("Bangladesh",11,13) %>%
  pack_rows("Brazil",14,15) %>%
  pack_rows("Vietnam",16,18) %>%
  pack_rows("Malawi",19,19) %>%
  pack_rows("TOTAL",20,20)
```


## Objectives

### Primary objective

To evaluate the performance of selected TB screening tests and combinations of such tests.

### Secondary objectives

* To identify test combinations that increase the proportion of people diagnosed with microbiologically confirmed TB.

* To demonstrate that combinations of current and newer TB tests can facilitate using these tests in locations where they are not currently available.


# Data simulation

In order to demonstrate the planned analyses and show computer code, we will simulate data like the one expected to be generated by the study.

```{r}
#| label: tbl-dataSim
#| tbl-cap: "10 random rows from the simulated data."

set.seed(123)

sensSpecMatHiv<-data.frame(
  sens=rnorm(n=7,mean=0.7,sd=0.02),
  spec=rnorm(n=7,mean=0.6,sd=0.01)
)
rownames(sensSpecMatHiv)<-c("Cameroon","Nigeria","Kenya","Bangladesh","Brazil","Vietnam","Malawi")

sensSpecMatW4ss<-data.frame(
  sens=rnorm(n=7,mean=0.80,sd=0.02),
  spec=rnorm(n=7,mean=0.65,sd=0.01)
)
rownames(sensSpecMatW4ss)<-c("Cameroon","Nigeria","Kenya","Bangladesh","Brazil","Vietnam","Malawi")

sensSpecMatSsm<-data.frame(
  sens=rnorm(n=7,mean=0.70,sd=0.02),
  spec=rnorm(n=7,mean=0.95,sd=0.01)
)
rownames(sensSpecMatSsm)<-c("Cameroon","Nigeria","Kenya","Bangladesh","Brazil","Vietnam","Malawi")

sensSpecMatCrp<-data.frame(
  sens=rnorm(n=7,mean=0.9,sd=0.02),
  spec=rnorm(n=7,mean=0.95,sd=0.01)
)
rownames(sensSpecMatCrp)<-c("Cameroon","Nigeria","Kenya","Bangladesh","Brazil","Vietnam","Malawi")

sensSpecMatLam<-data.frame(
  sens=rnorm(n=7,mean=0.82,sd=0.02),
  spec=rnorm(n=7,mean=0.9,sd=0.01)
)
rownames(sensSpecMatLam)<-c("Cameroon","Nigeria","Kenya","Bangladesh","Brazil","Vietnam","Malawi")

sensSpecMatPoolxpert<-data.frame(
  sens=rnorm(n=7,mean=0.92,sd=0.01),
  spec=rnorm(n=7,mean=0.97,sd=0.01)
)
rownames(sensSpecMatPoolxpert)<-c("Cameroon","Nigeria","Kenya","Bangladesh","Brazil","Vietnam","Malawi")

sensSpecMatCxr<-data.frame(
  sens=rnorm(n=7,mean=0.88,sd=0.025),
  spec=rnorm(n=7,mean=0.88,sd=0.02)
)
rownames(sensSpecMatCxr)<-c("Cameroon","Nigeria","Kenya","Bangladesh","Brazil","Vietnam","Malawi")

sensSpecMatXpert<-data.frame(
  sens=rnorm(n=7,mean=0.98,sd=0.005),
  spec=rnorm(n=7,mean=0.99,sd=0.005)
)
sensSpecMatXpert$sens[sensSpecMatXpert$sens>=0.99999]<-0.99999
sensSpecMatXpert$spec[sensSpecMatXpert$spec>=0.99999]<-0.99999
rownames(sensSpecMatXpert)<-c("Cameroon","Nigeria","Kenya","Bangladesh","Brazil","Vietnam","Malawi")


dfSim<-data.frame(
  PID=c(paste(sep="_","Ca",1:3300),paste(sep="_","Ni",1:3400),paste(sep="_","Ke",1:1400),paste(sep="_","Ba",1:3000),paste(sep="_","Br_Ar",1:500),paste(sep="_","Br_Ma",1:500),paste(sep="_","Vi",1:3000),paste(sep="_","Ma",1:1200)),
  region=c(rep("",3300+3400+1400+3000),rep("Aracaju",500),rep("Maceio",500),rep("",3000+1200)),
  country=c(rep("Cameroon",3300),rep("Nigeria",3400),rep("Kenya",1400),rep("Bangladesh",3000),rep("Brazil",1000),rep("Vietnam",3000),rep("Malawi",1200)),
  region=c(rep("",3300+3400+1400+3000),rep("Aracaju",500),rep("Maceio",500),rep("",3000+1200)),
  setting=c(rep("PHC",1000),rep("District",1000),rep("Informal settlements ACF",800),rep("Children",500),rep("PHC",700),rep("District",700),rep("Nomads",1000),rep("IDP / refugees",1000),rep("PHC",700),rep("District",700),rep("PHC",1000),rep("District",1000),rep("Informal settlements ACF",1000),rep("PHC",1000),rep("PHC",1000),rep("Informal settlements ACF",1500),rep("Children",500),rep("PHC",1200)),
  reference=c(rbinom(n=1000,size=1,prob=0.09),rbinom(n=1000,size=1,prob=0.08),rbinom(n=800,size=1,prob=0.05),rbinom(n=500,size=1,prob=0.04),rbinom(n=700,size=1,prob=0.20),rbinom(n=700,size=1,prob=0.20),rbinom(n=1000,size=1,prob=0.08),rbinom(n=1000,size=1,prob=0.12),rbinom(n=700,size=1,prob=0.09),rbinom(n=700,size=1,prob=0.20),rbinom(n=1000,size=1,prob=0.05),rbinom(n=1000,size=1,prob=0.03),rbinom(n=1000,size=1,prob=0.05),rbinom(n=500,size=1,prob=0.15),rbinom(n=500,size=1,prob=0.27),rbinom(n=1000,size=1,prob=0.02),rbinom(n=1500,size=1,prob=0.01),rbinom(n=500,size=1,prob=0.001),rbinom(n=1200,size=1,prob=0.08))
  ) %>%
  dplyr::mutate(
    age=rpois(n=n(),lambda=28),
    sex=sample(size=n(),c("Female","Male"),prob=c(0.5,0.5),replace=TRUE),
    hiv=case_when(reference==0~rbinom(n=n(),size=1,prob=1-sensSpecMatHiv[country,"spec"]),reference==1~rbinom(n=n(),size=1,prob=sensSpecMatHiv[country,"sens"])),
    w4ss=case_when(reference==0~rbinom(n=n(),size=1,prob=1-sensSpecMatW4ss[country,"spec"]),reference==1~rbinom(n=n(),size=1,prob=sensSpecMatW4ss[country,"sens"])),
    ssm=case_when(reference==0~rbinom(n=n(),size=1,prob=1-sensSpecMatSsm[country,"spec"]),reference==1~rbinom(n=n(),size=1,prob=sensSpecMatSsm[country,"sens"])),
    crp=case_when(reference==0~rbinom(n=n(),size=1,prob=1-sensSpecMatCrp[country,"spec"]),reference==1~rbinom(n=n(),size=1,prob=sensSpecMatCrp[country,"sens"])),
    lam=case_when(reference==0~rbinom(n=n(),size=1,prob=1-sensSpecMatLam[country,"spec"]),reference==1~rbinom(n=n(),size=1,prob=sensSpecMatLam[country,"sens"])),
    poolxpert=case_when(reference==0~rbinom(n=n(),size=1,prob=1-sensSpecMatPoolxpert[country,"spec"]),reference==1~rbinom(n=n(),size=1,prob=sensSpecMatPoolxpert[country,"sens"])),
    cxr=case_when(reference==0~rbinom(n=n(),size=1,prob=1-sensSpecMatCxr[country,"spec"]),reference==1~rbinom(n=n(),size=1,prob=sensSpecMatCxr[country,"sens"])),
    xpert=case_when(reference==0~rbinom(n=n(),size=1,prob=1-sensSpecMatXpert[country,"spec"]),reference==1~rbinom(n=n(),size=1,prob=sensSpecMatXpert[country,"sens"]))
) %>%
  dplyr::mutate(
    ageCentered=age-20,
  )

dfAge<-rcs(dfSim$ageCentered,parms=5)
colnames(dfAge)<-paste(sep="","ageSpline",1:4)
ageKnots<-attributes(dfAge)$parms

dfSim<-dfSim %>%
  dplyr::mutate(
    ageSpline1=as.vector(dfAge[,1]),
    ageSpline2=as.vector(dfAge[,2]),
    ageSpline3=as.vector(dfAge[,3]),
    ageSpline4=as.vector(dfAge[,4]),
  )

dfSim[sample(size=10,replace=F,x=1:nrow(dfSim)),] %>%
  kable(row.names=FALSE) %>%
  kable_styling(full_width=FALSE)

write.csv(dfSim,file=paste(sep="","simData_S4A_",gsub(Sys.Date(),pattern="-",replacement=""),".csv"),row.names=F)
```

@tbl-dataSim shows 10 random rows from the data frame containing all `r nrow(dfSim)` simulated observations.

# Statistical Analysis Plan

The R code to generate the results is embedded in this document. By default it is hidden, but can be displayed by clicking on the `Code` boxes on the right hand side.

## General considerations

The reporting of this study will be prepared in accordance with the STARD and [@bossuytEtal2015] and STROBE [@vonElmEtal2007] guidelines.

All continuous data variables will be summarized using the following descriptive statistics:

* N (size of relevant analysis population)
* n (size of analysis population without missing values)
* proportion of complete data for each variable (n/N)
* arithmetic mean (or geometric mean if more appropriate)
* standard deviation (SD)
* median
* 25th percentile value (P25), 75th percentile value (P75) and interquartile range (IQR)
* minimum and maximum (where relevant)

The proportion / percentage of observed levels will be reported for all binary and categorical measures. When appropriate, corresponding exact 95% confidence intervals (CIs) for proportions will be included.

For statistical test, a significance level of 5% will be used. All p-values will be reported to 3 decimal digits.

### Reporting conventions

P-values $\geq 0.001$ and $\leq 0.999$ will be reported to 3 decimal places; p-values less than 0.001 will be reported as "< 0.001". The mean, standard deviation, median, IQR and other statistics will be reported to one decimal place greater than the original data. Minimum and maximum values will use the same number of decimal places as the original data. Proportions will be presented as two decimal places; values greater than zero but $<0.01$ will be presented as "< 0.01". Percentages will be reported to 2 decimal places; values greater than zero but $<0.01\%$ will be presented as "< 0.01%"; values greater than $99.99\%$ but less than $100\%$ will be reported as "> 99.99%". Estimated parameters, not on the same scale as raw observations (e.g. regression coefficients) will be reported to 3 significant figures.

### Missing data

As all samples for the test diagnostics get taken at the only study visit, we expect only marginal missing data. Any missing data that does arise we expect to be due to technical reasons unrelated to the outcome of interest. However, we do expect that there will be some missing data, both for individual tests as well as for the reference standard and participant demographic and clinical data. Since we are adopting a Bayesian framework, we can easily incorporate missing data imputation as part of the MCMC process: by specifying distributions for all parameters of interest, the MCMC algorithm will consider any missing data to be no different than the unknown parameters of interest to our investigation. It will essentially impute a value for each sample at each iteration in the MCMC chains and incorporate the uncertainty of that imputation through the distribution building up over the full set of MCMC chains and iterations.


### Technical details

The R environment for statistical computing (v4.5.0 or later) will be used for all analyses.

The Bayesian models are implemented in JAGS, v4.3.2.

All analysis code will be made publicly available under an MIT or GNU GPL v3.0 license on GitHub.


## Primary objective analyses

For the primary objective we will estimate performance metrics for individual screening tests as well as combinations ("screening algorithms") of tests. Specifically, we will estimate and report for each test and each test combination:

* Sensitivity (Se)

* Specificity (Sp)

* Positive predictive value (PPV)

* Negative predictive value (NPV)

* Likelihood ratio for a positive test result (LR+) = Se/(1-Sp)

* Likelihood ratio for a negative test result (LR-) = (1-Se)/Sp

* Diagnostic yield per test (DYT), the proportion of positive tests

Our primary focus, from a performance point of view, will be on sensitivity and specificity, but there other metrics are of interest too, hence why we report them as well.

As we will use a Bayesian approach (see @sec-estimation), we will estimate posterior distributions for each of these parameters, for each evaluated algorithm in each setting. We will summarise these distributions using the posterior mean and the quantile-based Bayesian confidence interval given by the 2.5th and 97.5th percentiles of the posterior distribution.

In addition to providing tables with these posterior point estimates, we will also construct forest plots showing sensitivities and specificities for each algorithm.

### PCF / ICF / ACF analyses

Start 4 All populations use a mix of passive case finding (PCF), intensified case finding (ICF) and active finding (ACF) strategies. We will not distinguish between PCF and ICF for the purpose of the diagnostic performance evaluation; however for PCF/ICF, which are used in facility-based screening, we will distinguish between lowest level of care and district / referral level of care.

The modelling approach detailed below will be applied to each of the populations and settings for Start 4 All. What will differ however are the specific algorithms being evaluated for PCF/ICF on the one hand and ACF on the other (see further below for the list of algorithms evaluated).

### Thresholds for diagnostic tests: CRP and CXR-CAD {#sec-thresholds}

We will use the following thresholds to define positive/negative status for the following assays:

* Quantitative CRP:

  + Positive if CRP $\geq 5\, mg/L$, negative otherwise.

* Semi-quantitative CRP:

  + Positive if CRP-RDT is one of "10-40", "40-80", ">80", negative if CRP-RDT is "<10".

* CXR-CAD:

  + PCF/ICF: positive if CAD (Qure-ai) score $\geq 0.5$, negative otherwise,
  + ACF: positive if CAD (Qure.ai) score $\geq 0.3$, negative otherwise.

These are the threshold we will use for the primary analysis. For secondary analyses, we will use:

* Quantitative CRP with threshold $\geq 10\, mg/L$,
* CXR-CAD AI score with thresholds $\geq 0.3$ (in PCF/ICF) and $\geq 0.5$ (in ACF).

Note that for the primary analysis we will only use quantitative CRP, but sensitivity analyses will use semi-quantitative CRP and a hybrid of quantitative and semi-quantitative CRP (positive if either is positive).


### Stratification

All analyses specified in this SAP will be for adult participants only (defined as 15 or above, though due to local regulation some countries recruited 18 or above only). While Vietnam and Cameroon also recruited children (14 or below), there will be a separate SAP for the paediatric performance evluation.

All analyses, primary as well as secondary, will be stratified by:

* Country (but for facility-based we will also compute results pooled across countries).
* Facility-based (PCF/ICF) vs. key populations-based (ACF) recruitment.
* Within facility-based recruitment: PLWH vs people with negative or unknown HIV status (but we will also compute results for all participants regardless of HIV status).
* Within facility-based recruitment: lowest level of care vs. district level.
* Within key populations: recruitment via W4SS or CXR-CAD recruitment. We will also analysis a hybrid recruitment of either W4SS or CAD-CXR positive screening.

In summary, analyses (primary as well as secondary) will be done in the following groups:

1. ACF, W4SS positive screening (separately for each country and key population).
2. ACF, CAD-CXR positive screening (separately for each country and key population).
3. ACF, W4SS pr CAD-CXR positive screening (separately for each country and key population).
4. PCF/ICF, lowest level of care, all participants (separately per country and pooled across countries).
5. PCF/ICF, lowest level of care, PLWH (separately per country and pooled across countries).
6. PCF/ICF, lowest level of care, people with negative or unknown HIV status (separately per country and pooled across countries).
7. PCF/ICF, district level, all participants (separately per country and pooled across countries).
8. PCF/ICF, district level, PLWH (separately per country and pooled across countries).
9. PCF/ICF, district level, people with negative or unknown HIV status (separately per country and pooled across countries).


### Tuberculosis screening and diagnostic algorithms

For notation, to denote that we will use `-->` to refer to one assay followed by another assay and `|` to refer to assays being administered in parallel. In other words, combination `A | B --> C --> D` refers to a an algorithm where if either assay A or assay B gives a positive result (or both are positive) then proceed to assay C, and if that one is positive, then proceed to assay D (and record the outcome from that assay), else record a negative result.

#### PCF/ICF

We will distinguish between lowest level of care and district-level settings, but the algorithms evaluated are the same.

1. `Xpert Ultra with pooling --> Xpert Ultra`
2. `Xpert Ultra`
3. `LAM`
4. `CXR-CAD --> Xpert Ultra with pooling --> Xpert Ultra`
5. `CRP --> Xpert Ultra with pooling --> Xpert Ultra`
6. `LAM --> Xpert Ultra with pooling --> Xpert Ultra`
7. `CXR-CAD | CRP | LAM --> Xpert Ultra with pooling --> Xpert Ultra`
8. `CXR-CAD | LAM --> Xpert Ultra with pooling --> Xpert Ultra`
9. `CRP | LAM --> Xpert Ultra with pooling --> Xpert Ultra`
10. `CXR-CAD | CRP --> Xpert Ultra with pooling --> Xpert Ultra`
11. `CXR-CAD --> Xpert Ultra`
12. `CRP --> Xpert Ultra`
13. `LAM --> Xpert Ultra`
14. `CXR-CAD | CRP | LAM --> Xpert Ultra`
15. `CXR-CAD | LAM --> Xpert Ultra`
16. `CRP | LAM --> Xpert Ultra`
17. `CXR-CAD | CRP --> Xpert Ultra`
18. `CXR-CAD --> LAM`
19. `CRP --> LAM`
20. `CXR-CAD | CRP --> LAM`

#### 2. ACF

We will distinguish between W4SS positive, CAD-CXR positive and a hybrid approach (either of W4SS, CAD-CXR positive) entry into the study. Conditional on the first step, the algorithms evaluated differ slightly depending on whether or not the first step involved CXR-CAD.

##### ACF - W4SS entry

To note that these are the exact same algorithms as for PCF/ICF (both have W4SS entry into the study).

1. `Xpert Ultra with pooling --> Xpert Ultra`
2. `Xpert Ultra`
3. `LAM`
4. `CXR-CAD -> Xpert Ultra with pooling --> Xpert Ultra`
5. `CRP -> Xpert Ultra with pooling --> Xpert Ultra`
6. `LAM -> Xpert Ultra with pooling --> Xpert Ultra`
7. `CXR-CAD | CRP | LAM --> Xpert Ultra with pooling --> Xpert Ultra`
8. `CXR-CAD | LAM --> Xpert Ultra with pooling --> Xpert Ultra`
9. `CRP | LAM --> Xpert Ultra with pooling --> Xpert Ultra`
10. `CXR-CAD | CRP --> Xpert Ultra with pooling --> Xpert Ultra`
11. `CXR-CAD --> Xpert Ultra`
12. `CRP --> Xpert Ultra`
13. `LAM --> Xpert Ultra`
14. `CXR-CAD | CRP | LAM --> Xpert Ultra`
15. `CXR-CAD | LAM --> Xpert Ultra`
16. `CRP | LAM --> Xpert Ultra`
17. `CXR-CAD | CRP --> Xpert Ultra`
18. `CXR-CAD --> LAM`
19. `CRP --> LAM`
20. `CXR-CAD | CRP --> LAM`

##### ACF - CXR-CAD entry or (CXR-CAD or W4SS) entry

The algorithms evaluated for the CXR-CAD entry to the study in ACF or the hybrid approach where either a positive W4SS screening or a positive CXR-CAD makes a participant eligible for the study are the same.

1. `Xpert Ultra with pooling --> Xpert Ultra`
2. `Xpert Ultra`
3. `LAM`
4. `CRP --> Xpert Ultra with pooling --> Xpert Ultra`
5. `LAM --> Xpert Ultra with pooling --> Xpert Ultra`
6. `CRP | LAM --> Xpert Ultra with pooling --> Xpert Ultra`
7. `CRP --> Xpert Ultra`
8. `LAM --> Xpert Ultra`
9. `CRP | LAM --> Xpert Ultra`
10. `CRP --> LAM`

### Estimation {#sec-estimation}

We will estimate all of the probability parameters using Bayesian statistics. In Bayesian estimation, rather than calculating point estimates for each parameter of interest, the parameters are considered to be random variables and one estimates a distribution for each parameter. In Bayesian estimation, we estimate the posterior probability density of a given parameter; *posterior* because it is the best-fit distribution after we have observed data. The posterior distribution summarises our beliefs about the likely values for the parameter of interest and it is an update of our beliefs about the parameter before we had observed data -- the prior distribution. The data are generated by a stochastic process and we can compute the likelihood of the data under a distribution for this process. To summarise:

$$\mbox{posterior} = \mbox{prior} \times \mbox{likelihood}$$

For Phase 1 of the Start 4 All programme, the parameters we are mostly interested in sensitivities, specificities, PPVs, NPVs of individual tests and test combinations, conditional PPVs and NPVs and proportions of positive tests at each step of a test combination, TB prevalence) are all probability parameters. The exception will be the positive and negative likelihood ratios, LR+, LR-, but they can be directly derived from the sensitivities and specificieties. The natural parametric distribution for probability parameters is the beta distribution as it has support over the interval $[0,1]$.

A random variable $X$ follows a beta distribution with parameters $a,b$, $X\sim\beta(a,b)$, if

$$
p(x)=\begin{cases}
\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}     &\mbox{ if }x\in[0,1] \\
0             &\mbox{ otherwise}
\end{cases}
$$

where $B(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$.

The mean of this distribution is given by $E(X)=\frac{a}{a+b}$ and the variance is $Var(X)=\frac{a\cdot b}{(a+b)^2(a+b+1)}$. The mode is given by $\frac{a-1}{a+b-2}$, but this is only defined if $a,b>1$.

This means that for each probability parameter we plan to estimate within Start 4 All, we can assume a beta prior distribution, $p\sim\beta(a,b)$, where $p$ is the probability parameter of interest. The corresponding data likelihood will be a binomial likelihood $X\sim Bin(n,p)$, where $X$ is the number of positive results (possibly conditional on prior tests), $n$ the corresponding total number of observations (aka the denominator) and $p$ the same probability parameter as in the prior. One can show that the posterior will also be a beta distribution (since the beta is the conjugate prior for a binomial likelihood) with a closed form solution: $p|X=k\sim\beta(k+a,n-k+b)$. The closed-form solution is computationally fast to compute - no need to use Markov Chain Monte Carlo (MCMC) or Integrated Nested Lapace Approximation (INLA).

For prior, a weakly informative prior can be used, for example the Kerman prior $\beta(1/3,1/3)$ [@kerman2011] or the improper Haldane prior $\beta(0,0)$ [@haldane1948]. The latter has the advantage that it essentially does not provide any prior information and the posterior mean corresponds exactly to the sample mean. The drawback of the Haldane prior is, however, that where k=0 or k=n, it yields an improper posterior distribution. Another drawback is that JAGS, the MCMC sampler we are using, does not allow improper priors. For this reason, we will use the Kerman prior.

Specifically, with the Kerman prior, for a probability parameter of interest $p$, if we observe $X=k$ positive results out of $n$ total results, then the posterior distribution will be a beta distribution with parameters $k+a=k+1/3$ and $n-k+b=n-k+1/3$:

$$p|n,X=k\sim\beta(k+a,n-k+b)$$

The distributional parameters $\alpha=k+a, \beta=n-k+b$ for the posterior will then be directly fed into the health economic modelling to guarantee correct propagation of errors.

The data we will collect is expected to be incomplete - some individuals will have missing results for some of the tests and some may even lack a reference test result, meaning we would not know their TB status.

For these reasons, we will use a hybrid approach, using both the closed form result from above as well as Markov Chain Monte Carlo (MCMC) estimation and modelling directly the individual test results for each test for each participant from each country and setting in the study.

To note that, when we use MCMC, only a relatively small subset of probability parameters (TB prevalence, sensitivities and specificities of individual tests) will generate all of the data, and we can, in theory, derive any and all conditional probability parameters (e.g. P(positive LAM | positive CAD-CXR)) directly from this small subset. However this requires some assumptions, specifically that conditional on TB status, the different tests are independent. For Xpert and pooled Xpert, which use the same technology, this can easily be seen not to be true and so we will still derive all conditional parameters individually, but using the MCMC process to impute the missing data while incorporating the uncertainty of these imputations into our estimations.

All Bayesian models will be implemented in JAGS, we will run 4 chains, with each chain run for 3,000 adaptive iterations, 1,000 burn-in iterations and 12,000 iterations that are retained for statistical inference. Depending on MCMC diagnostics for mixing and non-convergence, we may run more chains for longer if needed.
(Note that the mock-ups in this SAP, for convenience, used only 4 chains, 1,000 adaptive iterations, 500 burn-in iterations and 2,000 iterations for inference.)

#### Per-country analysis

We will assume that the individual probability parameters are random variables. We will further assume that the set of sensitivity and specificity parameters for the different tests follow a non-trivial joint distribution and we will specify this in the MCMC model.

Let:

* $N$ be the number of participants in the study,
* $m=5$ be the number of tests / assays being evaluated,
* $T_i, i=1,\ldots,N$ be a binary variable recording the TB status of the $i^{th}$ study participant (for the purpose of Start 4 All we take this as the result of the microbiological confirmation / reference test),
* $X_{j,i}, j=\mbox{cxr, crp, lam, pXpert, sXpert}, i=1,\ldots,N$ be the individual test results for the different participants,
* $p_{se,j}$, $p_{sp,j}$ be the sensitivity and specificity of test $j$,
* $p_{TB}$ be the prevalence of TB in the population.

With this notation, we can write down the assumptions of the Bayesian MCMC estimation:

1. TB status is Bernoulli distributed.

$$T_i\sim\mbox{Bernoulli}(p_{TB})$$
with $\mbox{logit}(p_{TB})=\beta_0+\beta_{s}\cdot\mbox{sex}+\beta_{h}\cdot\mbox{hiv}+\mathbb{\beta}_a\cdot s(\mbox{age})$, assuming weakly informative $\cal{N}(0,10)$ priors for the $\beta_j$ parameters. $s()$ indicates a restricted cubic / natural spline with 5 knots and $\mathbb{\beta}_a$ is the corresponding vector of parameters. For the analyses stratified by HIV status, the HIV term is dropped from the linear predictor term.


2. Individual test results are Bernoulli distributed, conditional on TB status.

$$X_{j,i}\sim\left\{
\begin{array}{ll}
\mbox{Bernoulli}(p_{se,j}) & \quad\mbox{ if }T_i=1 \\
\mbox{Bernoulli}(1-p_{sp,j}) & \quad\mbox{ if }T_i=0
\end{array}
\right.
$$

3. Sensitivities, specificities are specified as the inverse logits of real scalars.

$$
\begin{array}{lll}
p_{se,j} &=& \mbox{logit}^{-1}(z_{se,j}) \\
p_{sp,j} &=& \mbox{logit}^{-1}(z_{sp,j})
\end{array}
$$

4. These, or in other words, the logits of sensitivities, specificities, follow a non-trivial joint multivariate normal distribution.

$$\mathbb{z}=(z_{se,w4ss},z_{sp,w4ss},z_{se,cxr},z_{sp,cxr},\ldots,z_{se,sXpert},z_{sp,sXpert})\sim MV\cal{N}(\mathbb{\mu},\mathbb{\Sigma})$$

5. Priors for the means of the logits are weakly informative.

$$
\begin{array}{lll}
\mu_{se,j}\sim\cal{N}(0,10) \\
\mu_{sp,j}\sim\cal{N}(0,10)
\end{array}
$$

6. Prior for the covariance matrix follow a satndard prior distribution for covariance matrices.

$$\Sigma\sim\mbox{invWishart}(\Lambda,\eta)\qquad \Lambda=I,\; \eta=2\cdot m+1$$

7. Assuming that all dependencies between tests are captured through the above joint distribution of sensitivities and specificities and through conditioning on TB status, then all other, conditional, probability parameters can be derived from the sensitivity and specificity parameters. However, as explained earlier, the assumption of conditional independence is most likely unreasonable. For this reason, for each conditional parameter $\pi|\mathbf{T}$ (where $\mathbf{T}$ denote all test results upon which the probability parameter is conditional on), we will assume:

    + $\pi|\mathbf{T}\sim\beta(a=K+1/3,b=M-K+1/3)$

    + $M,K$ are the random variables for the count of records for which the parameter is being evaluated (M) and the count of positive results (K) respectively and they are derived from the MCMC process (specifically $T_i, X_{i,j}$).


8. This final distribution for $\pi|\mathbf{T}$ is not necessarily a beta distribution itself (since M and K are random themselves). For the performance evaluation, we will report the distribution as is. However to feed this posterior distribution into the health economic modelling, we will assume that it can be approximated by one. We will use quantile-matching to identify the best-fitting beta distribution for the resulting empirical distribution from the MCMC output. The beta distribution parameters from this best-fit beta distribution are then entered into the health economic modelling.


#### Pooled PCF/ICF analysis at lowest level of care

For facility-based screening, we will pool the data from all Start 4 All partner countries to estimate a single set of performance metrics at i) lowest level of care and ii) district-level care.

There is a need to account for country-level variation in the estimation procedure described above.

Let:

* $N$ be the number of participants in the study,
* $m=5$ be the number of tests / assays being evaluated,
* $T_{l,i}$ be a binary variable recording the TB status of the $i^{th}$ study participant from country $l$,
* $X_{l,j,i}$ be the individual test result for participant $i$ from country $l$ for assay $j$,
* $p_{se,l,j}$, $p_{sp,l,j}$ be the sensitivity and specificity of test $j$ in country $l$,
* $p_{TB,l}$ be the prevalence of TB in the population in country $l$.

where the indices ${i,j,l}$ iterate over:

* $l=\mbox{Bangladesh, Brazil, Cameroon, Kenya, Malawi, Nigeria, Vietnam}$,
* $j=\mbox{cxr, crp, lam, pXpert, sXpert}$,
* $i=1,\ldots,N$

With this notation, we can write down the assumptions for the Bayesian MCMC estimation:

1. TB status is Bernoulli distributed.

$$T_{l,i}\sim\mbox{Bernoulli}(p_{TB,l})$$
with $\mbox{logit}(p_{TB})=\beta_0+\beta_{s}\cdot\mbox{sex}+\beta_{h}\cdot\mbox{hiv}+\mathbb{\beta}_a\cdot s(\mbox{age})$, assuming weakly informative $\cal{N}(0,10)$ priors for the $\beta_j$ parameters. $s()$ indicates a restricted cubic / natural spline with 5 knots and $\mathbb{\beta}_a$ is the corresponding vector of parameters. For the analyses stratified by HIV status, the HIV term is dropped from the linear predictor term.

2. Individual test results are Bernoulli distributed, conditional on TB status.

$$
X_{l,j,i}\sim\left\{
\begin{array}{ll}
\mbox{Bernoulli}(p_{se,l,j}) & \quad\mbox{ if }T_{l,i}=1 \\
\mbox{Bernoulli}(1-p_{sp,l,j}) & \quad\mbox{ if }T_{l,i}=0
\end{array}
\right.
$$

3. Sensitivities, specificities are specified as the inverse logits of real scalars.

$$
\begin{array}{lll}
p_{se,l,j} &=& \mbox{logit}^{-1}(z_{se,j}+\tau_{l,j}) \\
p_{sp,l,j} &=& \mbox{logit}^{-1}(z_{sp,j}+\nu_{l,j})
\end{array}
$$

where $\tau_{l,j}, \nu_{l,j}$ are country-level random effects.


4. These, or in other words, the logits of sensitivities, specificities, follow a non-trivial joint multivariate normal distribution.

$$
\mathbb{z}=(z_{se,w4ss},z_{sp,w4ss},z_{se,cxr},z_{sp,cxr},\ldots,z_{se,sXpert},z_{sp,sXpert})\sim MV\cal{N}(\mathbb{\mu},\mathbb{\Sigma})
$$

5. Priors for the means of the logits are weakly informative.

$$
\begin{array}{lll}
\mu_{se,j}\sim\cal{N}(0,10) \\
\mu_{sp,j}\sim\cal{N}(0,10)
\end{array}
$$

6. Prior for the covariance matrix follow a standard prior distribution for covariance matrices.

$$
\Sigma\sim\mbox{invWishart}(\Lambda,\eta)\qquad \Lambda=I,\; \eta=2\cdot m+1
$$

7. Assuming that all dependencies between tests are captured through the above joint distribution of sensitivities and specificities and through conditioning on TB status, then all other, conditional, probability parameters can be derived from the sensitivity and specificity parameters. However, as explained earlier, the assumption of conditional independence is most likely unreasonable. For this reason, for each conditional parameter $\pi|\mathbf{T}$ (where $\mathbf{T}$ denote all test results upon which the probability parameter is conditional on), we will assume:

    + $\pi|\mathbf{T}\sim\beta(a=K+1/3,b=M-K+1/3)$

    + $M,K$ are the random variables for the count of records for which the parameter is being evaluated (M) and the count of positive results (K) respectively and they are derived from the MCMC process (specifically $T_{l,i}, X_{l,j,i}$).


8. This final distribution for $\pi|\mathbf{T}$ is not necessarily a beta distribution itself (since M and K are random themselves). For the performance evaluation, we will report the distribution as is. However to feed this posterior distribution into the health economic modelling, we will assume that it can be approximated by one. We will use quantile-matching to identify the best-fitting beta distribution for the resulting empirical distribution from the MCMC output. The beta distribution parameters from this best-fit beta distribution are then entered into the health economic modelling.


#### Model diagnostics

For every Bayesian model fitted, we will check for signs of non-convergence of the MCMC algorithm. Specifically we will:

* Inspect trace plots for all parameters (and, if needed, increase the number of chains and/or iterations per chain).

* Compute Gelman-Rubin potential scale reduction factors for all parameters (and, if needed, increase the number of iterations).

* Compute effect sample sizes for every parameter (and, if needed, increase the number of iterations).

* Compute posterior predictive checks to assess the goodness of fit of the model.


## Secondary analyses

These will include:

* Repeating the main analysis but with alternative threshold for quantitative CRP and CXR-CAD AI score (see @sec-thresholds).

* Using a composite reference standard of culture and molecular testing.

* Modelling performance as a function of TB prevalence.


## Sensitivity analyses

We will explore (see @sec-thresholds) using semi-quantitative CRP instead of quantitative CRP as well as a hybrid of quantitative and semi-quantitative CRP.


## Mock results

This version of the SAP includes some mock-ups but not yet all mock-ups as they needed to be re-done completely given the change in algorithms being evaluated. While the analysis plan itself is complete, full mock-ups will only be included in a future version.

```{r primAnalBayes}
#| message: false
#| warning: false

parsVectW4ss<-as.vector(unlist(read.csv(header=F,"../../output/20250526/parameters_forRscript_w4ss.txt")[1,]))
parsVectCxr<-as.vector(unlist(read.csv(header=F,"../../output/20250526/parameters_forRscript_cxr.txt")[1,]))

parsListW4ss.ab<-character(0)
for(par in parsVectW4ss){
  if(par=="ptb"){
    parsListW4ss.ab<-c(parsListW4ss.ab,"ptb.a","ptb.b")
  }else{
    tmp<-unlist(strsplit(split="\\.",par))
    parsListW4ss.ab<-c(parsListW4ss.ab,paste(sep=".",tmp[1],"a",paste(collapse=".",tmp[-1])),paste(sep=".",tmp[1],"b",paste(collapse=".",tmp[-1])))
  }
}

parsListCxr.ab<-character(0)
for(par in parsVectCxr){
  if(par=="ptb"){
    parsListCxr.ab<-c(parsListCxr.ab,"ptb.a","ptb.b")
  }else{
    tmp<-unlist(strsplit(split="\\.",par))
    parsListCxr.ab<-c(parsListCxr.ab,paste(sep=".",tmp[1],"a",paste(collapse=".",tmp[-1])),paste(sep=".",tmp[1],"b",paste(collapse=".",tmp[-1])))
  }
}

jagsFileW4ss<-"../../scripts/S4A_jagsPerformanceModel_entryW4SS_20250526_clean.jags"
jagsFileW4ssNoHiv<-"../../scripts/S4A_jagsPerformanceModel_entryW4SS_20250526_clean.noHIVstatus.jags"
jagsFileCxr<-"../../scripts/S4A_jagsPerformanceModel_entryCXR_20250526_clean.jags"
jagsFileCxr.NoHiv<-"../../scripts/S4A_jagsPerformanceModel_entryCXR_20250526_clean.noHIVstatus.jags"
jagsFileW4ssPooled<-"../../scripts/S4A_jagsPerformanceModel_entryW4SS_pooledFacility_20250526_clean.jags"
jagsFileW4ssPooled.NoHiv<-"../../scripts/S4A_jagsPerformanceModel_entryW4SS_pooledFacility_20250526_clean.noHIVstatus.jags"

nChains<-4
nAdapt<-1000 # 3000 in full analysis
nBurn<-500 # 1000 in full analysis
nIter<-2000 # 12000 in full analysis

# define the parameters to extract from the MCMC run
parsVectW4ssNoLR<-parsVectW4ss
parsVectW4ss<-c(parsVectW4ss,gsub(pattern="pse",replacement="lrp",grep(value=TRUE,pattern="pse",parsVectW4ss)),gsub(pattern="pse",replacement="lrm",grep(value=TRUE,pattern="pse",parsVectW4ss)))

parsVectCxrNoLR<-parsVectCxr
parsVectCxr<-c(parsVectCxr,gsub(pattern="pse",replacement="lrp",grep(value=TRUE,pattern="pse",parsVectCxr)),gsub(pattern="pse",replacement="lrm",grep(value=TRUE,pattern="pse",parsVectCxr)))

# helper functions to process MCMC output
ssBetaPars<-function(abPars, probs, parData, alpha = 0.05) {
  res<-sum((qbeta(probs,abPars[1],abPars[2])-quantile(parData,probs=probs))^2)
  return(res)
}

identifyBetaPars<-function(probs=c(0.025,0.25,0.5,0.75,0.975), parData, maxiter = 1000){
  if (length(probs)<2 | sum(probs<0 | probs>1)>0) {
    stop("There need to be at least 2 probs parameters and the probs parameters need all to be contained within [0,1].")
  }
  
  m<-mean(parData)
  s<-var(parData)
  initPars<-c(m*(m*(1-m)/s-1),(1-m)*(m*(1-m)/s-1))
  
  res <- suppressWarnings(optim(fn = ssBetaPars, probs=probs, parData=parData, par = initPars, control = list(maxit = maxiter), alpha = alpha))
  if (res$convergence != 0) {
    stop("optim() as called by identifyBetaPars() failed to converge.")
  }
  return(res$par)
}

getBetaDistsFromMCMC<-function(mcmcPars,doPlot=FALSE,plotFile="s4a_mcmcPars.pdf"){
  parsAll<-colnames(mcmcPars[[1]])
  pars<-parsAll[!grepl(parsAll,pattern="lrm|lrp") & !(parsAll %in% c(paste(sep="","b",0:2),paste(sep="","bs",1:4)))]
  nChains<-length(mcmcPars)
  
  g<-list()
  
  resDf<-data.frame(
    par=pars,
    a=NA,
    b=NA,
    distMean=NA,
    dataMean=NA
  )
  
  parsDf<-mcmcPars[[1]]
  if(nChains>1){
    for(j in 2:nChains){
      parsDf<-rbind(parsDf,mcmcPars[[j]])
    }
  }
  
  for(par in pars){
    tmpPars<-identifyBetaPars(parData=parsDf[,par])
    resDf$a[resDf$par==par]<-tmpPars[1]
    resDf$b[resDf$par==par]<-tmpPars[2]
    resDf$dataMean[resDf$par==par]<-mean(parsDf[,par])
  }
  
  if(doPlot){
    pdf(width=4*2,height=4*ceiling(length(pars)/2),file=plotFile)
    par(mfrow=c(ceiling(length(pars)/2),2))
    for(par in parsAll){
      hist(freq=FALSE,parsDf[,par],breaks=100,xlab=par,ylab="density")
      if(par %in% pars){
        xx<-seq(min(parsDf[,par]),max(parsDf[,par]),length=1e4)
        yy<-dbeta(xx,resDf$a[resDf$par==par],resDf$b[resDf$par==par])
        lines(xx,yy,lwd=2,col="steelblue")
      }
    }
    dev.off()
  }
  
  resDf$distMean<-resDf$a/(resDf$a+resDf$b)
  
  return(resDf)
}

# analysis function
analysisFunBayesBetaPars<-function(dat,c,r,s,doPlot=FALSE,plotFile="s4a_mcmcPars.pdf",nChains=nChains,nAdapt=nAdapt,nBurn=nBurn,nIter=nIter,jagsFile,parsVect){
  # dat = data frame with the data; requires columns country, region, setting, reference, hiv, w4ss, crp, lam, poolxpert, cxr, xpert
  # c = country
  # r = region
  # s = setting
  # doPlot = (logical) idnicatees whether or not to plot the MCMC trace plots and posterior distributions; defaults to FALSE
  # plotFile = filename for the graph (if doPlot==TRUE)
  # nChains, nAdapt, nBurn, nIter = MCMC parameters (number of chains, adaptation iterations, burn-in iterations, MCMC iterations)
  # jagsFile = filename with path to the JAGS model file
  # parsVect = vector with parameters to extract from / monitor during the MCMC sampling
  
  # prepare the data
  dfTmp<-dat %>%
    dplyr::filter(country==c & region==r & setting==s)
  
  datJags<-list(
    N=nrow(dfTmp),
    hiv=dfTmp$hiv,
    sexM=ifelse(dfTmp$sex=="Male",1,0),
    ageSpline1=dfTmp$ageSpline1,
    ageSpline2=dfTmp$ageSpline2,
    ageSpline3=dfTmp$ageSpline3,
    ageSpline4=dfTmp$ageSpline4,
    reference=dfTmp$reference,
    crp=dfTmp$crp,
    cxr=dfTmp$cxr,
    lam=dfTmp$lam,
    poolxpert=dfTmp$poolxpert,
    xpert=dfTmp$xpert,
    ssm=dfTmp$ssm
  )
  
  # run the JAGS model
  jagsModel <- jags.model(jagsFile, data=datJags, n.chains = nChains, n.adapt = nAdapt, quiet = TRUE)
  update(jagsModel,nBurn)
  parsModel<-coda.samples(model=jagsModel,variable.names=parsVect,n.iter = nIter, na.rm=FALSE)
  
  # process the MCMC results
  betaPars<-getBetaDistsFromMCMC(mcmcPars=parsModel,doPlot=doPlot,plotFile=plotFile)
  
  # return the results
  return(list(mcmcObj=parsModel,betaPars=betaPars))
}

analysisFunBayesBetaParsPool<-function(dat,set,doPlot=FALSE,plotFile="s4a_mcmcPars_pool.pdf",nChains=nChains,nAdapt=nAdapt,nBurn=nBurn,nIter=nIter,jagsFile,parsVect){
  # dat = data frame with the data; requires columns country, region, setting, reference, hiv, w4ss, crp, lam, poolxpert, cxr, xpert
  # set = PHC or District
  # doPlot = (logical) idnicatees whether or not to plot the MCMC trace plots and posterior distributions; defaults to FALSE
  # plotFile = filename for the graph (if doPlot==TRUE)
  # nChains, nAdapt, nBurn, nIter = MCMC parameters (number of chains, adaptation iterations, burn-in iterations, MCMC iterations)
  # jagsFile = filename with path to the JAGS model file
  # parsVect = vector with parameters to extract from / monitor during the MCMC sampling
  
  # prepare the data
  dfTmp<-dat %>%
    dplyr::filter(setting==set) %>%
    dplyr::mutate(
      site=as.integer(factor(
       paste(sep="",country,region) 
      ))
    )
  
  datJags<-list(
    N=nrow(dfTmp),
    nsites=length(unique(dfTmp$site)),
    site=dfTmp$site,
    hiv=dfTmp$hiv,
    ageSpline1=dfTmp$ageSpline1,
    ageSpline2=dfTmp$ageSpline2,
    ageSpline3=dfTmp$ageSpline3,
    ageSpline4=dfTmp$ageSpline4,
    sexM=ifelse(dfTmp$sex=="Male",1,0),
    reference=dfTmp$reference,
    crp=dfTmp$crp,
    cxr=dfTmp$cxr,
    lam=dfTmp$lam,
    poolxpert=dfTmp$poolxpert,
    xpert=dfTmp$xpert,
    ssm=dfTmp$ssm
  )
  
  # run the JAGS model
  jagsModel <- jags.model(jagsFile, data=datJags, n.chains = nChains, n.adapt = nAdapt, quiet = TRUE)
  update(jagsModel,nBurn)
  parsModel<-coda.samples(model=jagsModel,variable.names=parsVect,n.iter = nIter, na.rm=FALSE)
  
  # process the MCMC results
  betaPars<-getBetaDistsFromMCMC(mcmcPars=parsModel,doPlot=doPlot,plotFile=plotFile)
  
  # return the results
  return(list(mcmcObj=parsModel,betaPars=betaPars))
}

getLRpLRm<-function(mcmcObj){
  nChains<-length(mcmcObj)
  
  for(j in 1:nChains){
    pars<-colnames(mcmcObj[[j]])
    pars<-gsub(pattern="pse",replacement="",pars[grepl(pattern="pse.",pars)])
    
    x.thin<-thin(mcmcObj[[j]])
    x.start<-start(mcmcObj[[j]])
    x.end<-end(mcmcObj[[j]])
    
    for(par in pars){
      pse<-mcmcObj[[j]][,paste(sep="","pse",par)]
      psp<-mcmcObj[[j]][,paste(sep="","psp",par)]
      
      mcmcObj[[j]]<-as.data.frame(mcmcObj[[j]]) %>%
        dplyr::mutate(
          !!paste(sep="","lrp",par) := pse/(1-ifelse(psp<0.99999,psp,0.99999)),
          !!paste(sep="","lrm",par) := (1-pse)/psp
        )
    }
    
    mcmcObj[[j]]<-mcmc(data=as.matrix(mcmcObj[[j]]),start=x.start,end=x.end,thin=x.thin)
  }
  
  return(mcmcObj)
}

gr<-expand.grid(c("pse","psp","ppv","npv","dyt","posProp","lrm","lrp"),c("W4ss","Crp","Cxr","Lam","Poolxpert","Xpert","Ssm"))
parsVectPrim<-apply(MARGIN=1,FUN=paste,gr,collapse=".")
```

### Descriptive statistics and summary statistics

We will summarise the collected data in a simple table as shown on @tbl-dataSummary.

```{r}
#| label: tbl-dataSummary
#| tbl-cap: "Summary of the study data."

pTB<-mean(dfSim$reference,na.rm=T)
pTBCI<-binom.test(n=sum(!is.na(dfSim$reference)),x=sum(dfSim$reference,na.rm=TRUE))$conf.int
n<-nrow(dfSim)
nMissRef<-sum(is.na(dfSim$reference))
nMissAnyDiag<-sum(rowSums(is.na(dfSim[,c("reference","w4ss","crp","cxr","lam","xpert","poolxpert","ssm")]))>0)

dfSum<-data.frame(
  country="Any",
  setting="Any",
  n=n,
  nWithReference=sum(!is.na(dfSim$reference)),
  nTB=sum(dfSim$reference,na.rm=T),
  pTB=paste(sep="",format(nsmall=2,round(digits=2,100*pTB)),"% (",format(nsmall=2,round(digits=2,100*pTBCI[1])),"%,",format(nsmall=2,round(digits=2,100*pTBCI[2])),"%)"),
  missingRef=paste(sep="",nMissRef," (",format(nsmall=2,round(digits=2,100*nMissRef/n)),"%)"),
  missingAnyDiag=paste(sep="",nMissAnyDiag," (",format(nsmall=2,round(digits=2,100*nMissAnyDiag/n)),"%)")
)

dfSumStratified<-dfSim %>%
  dplyr::filter(country!="" & !is.na(setting)) %>%
  dplyr::group_by(country,setting) %>%
  dplyr::summarise(
    n=n(),
    nWithReference=sum(!is.na(reference)),
    nTB=sum(reference,na.rm=TRUE),
    pTB=ifelse(sum(!is.na(reference)>0),paste(sep="",format(nsmall=2,round(digits=2,100*mean(reference,na.rm=TRUE))),"%"," (",format(nsmall=2,round(digits=2,100*binom.test(x=sum(reference,na.rm=TRUE),n=sum(!is.na(reference)))$conf.int[1])),"%,",format(nsmall=2,round(digits=2,100*binom.test(x=sum(reference,na.rm=TRUE),n=sum(!is.na(reference)))$conf.int[2])),"%)"),NA),
    missingRef=paste(sep="",sum(is.na(reference))," (",format(nsmall=2,round(digits=2,100*sum(is.na(reference))/n())),"%)"),
    missingAnyDiag=paste(sep="",sum(is.na(reference+w4ss+crp+cxr+lam+xpert+poolxpert+ssm))," (",format(nsmall=2,round(digits=2,100*sum(is.na(reference+w4ss+crp+cxr+lam+xpert+poolxpert+ssm))/n())),"%)"),
    .groups="drop"
  ) 

dfSum<-rbind(dfSum,dfSumStratified)

dfSum %>%
  kable(col.names=c("Country","Setting","n","n (with available reference)","TB cases (culture)","TB prevalence (95% CI)","Culture not available (%)","Observations with at least one diagnostic missing (%)")) %>%
  kable_styling(full_width=FALSE)
```

We will also tabulate the performance of individual tests per country and setting for two different reference standards (culture or Xpert Ultra).

```{r}
#| label: tbl-perfSummary
#| tbl-cap: "Summary of the performance of individual tests compared against culture and Xpert Ultra."

gr<-expand.grid(c("culture","xpert"),c("All","PLWithHIV","PLWithoutHIV"),unique(paste(sep="_",dfSim$country,dfSim$region,dfSim$setting)))
gr[,1]<-as.character(gr[,1])
gr[,2]<-as.character(gr[,2])
gr[,3]<-as.character(gr[,3])

dfPerf<-data.frame(
  Country=NA,
  Region=NA,
  Setting=NA,
  CRS=gr[,3],
  Reference=gr[,1],
  Population=gr[,2],
  Prevalence=NA,
  cultNegXpertNeg=NA,
  cultPosXpertNeg=NA,
  cultNegXpertPos=NA,
  cultPosXpertPos=NA,
  cxr_sens=NA,
  cxr_spec=NA,
  crp_sens=NA,
  crp_spec=NA,
  lam_sens=NA,
  lam_spec=NA,
  ssm_sens=NA,
  ssm_spec=NA,
  poolxpert_sens=NA,
  poolxpert_spec=NA,
  xpert_sens=NA,
  xpert_spec=NA
)

for(j in 1:nrow(dfPerf)){
  dfPerf$Country[j]<-unlist(strsplit(split="_",dfPerf$CRS[j]))[1]
  dfPerf$Region[j]<-unlist(strsplit(split="_",dfPerf$CRS[j]))[2]
  dfPerf$Setting[j]<-unlist(strsplit(split="_",dfPerf$CRS[j]))[3]
  
  dfTmp<-dfSim %>%
    dplyr::filter(country==dfPerf$Country[j] & region==dfPerf$Region[j] & setting==dfPerf$Setting[j])
  
  if(dfPerf$Population[j]=="PLWithHIV"){
    dfTmp<-dfTmp %>%
      dplyr::filter(!is.na(hiv) & hiv==1)
  }else if(dfPerf$Population[j]=="PLWithoutHIV"){
    dfTmp<-dfTmp %>%
      dplyr::filter(!is.na(hiv) & hiv==0)
  }
  
  if(dfPerf$Reference[j]=="culture"){refTmp<-dfTmp$reference}else{refTmp<-dfTmp$xpert}
  
  k<-sum(na.rm=TRUE,refTmp)
  n<-sum(!is.na(refTmp))
  
  dfPerf$Prevalence[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*k/n)),"% (",k,"/",n,")")
  
  dfPerf$cultNegXpertNeg[j]<-sum(!is.na(dfTmp$reference) & !is.na(dfTmp$xpert) & dfTmp$reference==0 & dfTmp$xpert==0)
  dfPerf$cultPosXpertNeg[j]<-sum(!is.na(dfTmp$reference) & !is.na(dfTmp$xpert) & dfTmp$reference==1 & dfTmp$xpert==0)
  dfPerf$cultNegXpertPos[j]<-sum(!is.na(dfTmp$reference) & !is.na(dfTmp$xpert) & dfTmp$reference==0 & dfTmp$xpert==1)
  dfPerf$cultPosXpertPos[j]<-sum(!is.na(dfTmp$reference) & !is.na(dfTmp$xpert) & dfTmp$reference==1 & dfTmp$xpert==1)
  
  #CXR
  tp<-sum(na.rm=TRUE,refTmp*dfTmp$cxr)
  fp<-sum(na.rm=TRUE,(1-refTmp)*dfTmp$cxr)
  tn<-sum(na.rm=TRUE,(1-refTmp)*(1-dfTmp$cxr))
  fn<-sum(na.rm=TRUE,refTmp*(1-dfTmp$cxr))
  dfPerf$cxr_sens[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tp/(tp+fn))),"% (",tp,"/",tp+fn,")")
  dfPerf$cxr_spec[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tn/(tn+fp))),"% (",tn,"/",tn+fp,")")
  
  #CRP
  tp<-sum(na.rm=TRUE,refTmp*dfTmp$crp)
  fp<-sum(na.rm=TRUE,(1-refTmp)*dfTmp$crp)
  tn<-sum(na.rm=TRUE,(1-refTmp)*(1-dfTmp$crp))
  fn<-sum(na.rm=TRUE,refTmp*(1-dfTmp$crp))
  dfPerf$crp_sens[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tp/(tp+fn))),"% (",tp,"/",tp+fn,")")
  dfPerf$crp_spec[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tn/(tn+fp))),"% (",tn,"/",tn+fp,")")
  
  #LAM
  tp<-sum(na.rm=TRUE,refTmp*dfTmp$lam)
  fp<-sum(na.rm=TRUE,(1-refTmp)*dfTmp$lam)
  tn<-sum(na.rm=TRUE,(1-refTmp)*(1-dfTmp$lam))
  fn<-sum(na.rm=TRUE,refTmp*(1-dfTmp$lam))
  dfPerf$lam_sens[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tp/(tp+fn))),"% (",tp,"/",tp+fn,")")
  dfPerf$lam_spec[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tn/(tn+fp))),"% (",tn,"/",tn+fp,")")
  
  #SSM
  tp<-sum(na.rm=TRUE,refTmp*dfTmp$ssm)
  fp<-sum(na.rm=TRUE,(1-refTmp)*dfTmp$ssm)
  tn<-sum(na.rm=TRUE,(1-refTmp)*(1-dfTmp$ssm))
  fn<-sum(na.rm=TRUE,refTmp*(1-dfTmp$ssm))
  dfPerf$ssm_sens[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tp/(tp+fn))),"% (",tp,"/",tp+fn,")")
  dfPerf$ssm_spec[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tn/(tn+fp))),"% (",tn,"/",tn+fp,")")
  
  #XPERT POOLED
  tp<-sum(na.rm=TRUE,refTmp*dfTmp$poolxpert)
  fp<-sum(na.rm=TRUE,(1-refTmp)*dfTmp$poolxpert)
  tn<-sum(na.rm=TRUE,(1-refTmp)*(1-dfTmp$poolxpert))
  fn<-sum(na.rm=TRUE,refTmp*(1-dfTmp$poolxpert))
  dfPerf$poolxpert_sens[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tp/(tp+fn))),"% (",tp,"/",tp+fn,")")
  dfPerf$poolxpert_spec[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tn/(tn+fp))),"% (",tn,"/",tn+fp,")")
  
  #XPERT SINGLE
  tp<-sum(na.rm=TRUE,refTmp*dfTmp$xpert)
  fp<-sum(na.rm=TRUE,(1-refTmp)*dfTmp$xpert)
  tn<-sum(na.rm=TRUE,(1-refTmp)*(1-dfTmp$xpert))
  fn<-sum(na.rm=TRUE,refTmp*(1-dfTmp$xpert))
  dfPerf$xpert_sens[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tp/(tp+fn))),"% (",tp,"/",tp+fn,")")
  dfPerf$xpert_spec[j]<-paste(sep="",format(nsmall=1,round(digits=1,100*tn/(tn+fp))),"% (",tn,"/",tn+fp,")")
  
  rm(list=c("dfTmp","tp","tn","fp","fn"))
}

dfPerf$CRS<-gsub(dfPerf$CRS,pattern="_+",replacement=" ")

dfPerf<-dfPerf[!grepl(pattern="NaN",dfPerf$Prevalence),]
dfPerf<-dfPerf[dfPerf$cultNegXpertNeg+dfPerf$cultPosXpertNeg+dfPerf$cultNegXpertPos+dfPerf$cultPosXpertPos>=5,]

for(i in 1:nrow(dfPerf)){
  for(j in 1:ncol(dfPerf)){
    if(grepl(pattern="NaN",dfPerf[i,j])){dfPerf[i,j]<-NA}
  }
}

dfPerf %>%
  dplyr::select(!c(Country,Region,Setting,CRS)) %>%
  kable(row.names = FALSE,col.names=c("Reference","Population","TB prevalence","-/-","+/-","-/+","+/+",rep(c("sensitivity","specifity"),6))) %>%
  add_header_above(c(" "=1," "=1," "=1,"Culture/Xpert"=4,"CXR"=2,"CRP"=2,"LAM"=2,"SSM"=2,"Pooled Xpert"=2,"Xpert Ultra"=2)) %>%
  kableExtra::group_rows(index=table(fct_inorder(dfPerf$CRS))) %>%
  kable_styling(full_width=FALSE)
```


#### PCF / ICF lowest level of care

```{r analysis}
#| eval: false

# prepare output data frame
dfResPrimBayes<-data.frame(
  country=c(rep("Cameroon",6),rep("Nigeria",8),rep("Kenya",2),rep("Bangladesh",5),rep("Brazil",2),rep("Viet Nam",5),rep("Malawi",1),"Pooled","Pooled"),
  region=c(rep("",6),c(rep("ZRC",2),rep("JHF",6)),rep("",2+5),"Aracaju","Maceio",rep("",5+1),rep("",2)),
  setting=c("PHC","District",rep("Nomads",3),"Children","PHC","District",rep("Nomads",3),rep("IDP / refugees",3),"PHC","District","PHC","District",rep("Informal settlements",3),"PHC","PHC","District",rep("Informal settlements",3),"Children","PHC","PHC","District"),
  entryToStudy=c(rep("W4SS",2),"W4SS","CXR","W4SS | CXR","",rep("W4SS",2),"W4SS","CXR","W4SS | CXR","W4SS","CXR","W4SS | CXR",rep("W4SS",2),rep("W4SS",2),"W4SS","CXR","W4SS | CXR",rep("W4SS",2),"W4SS","W4SS","CXR","W4SS | CXR","","W4SS",rep("W4SS",2))
) %>%
  dplyr::filter(setting!="Children")

allRes<-list()

printResults<-TRUE
checkW4ssFacility<-FALSE
checkW4ssAcf<-FALSE
checkW4ssCxrAcf<-FALSE
checkCxrAcf<-FALSE

outPrefix<-"mockData_AnalysisResults_"

for(i in 1:(nrow(dfResPrimBayes)-2)){
  print(i)
  
  if(dfResPrimBayes$setting[i] %in% c("PHC","District") & dfResPrimBayes$country[i]!="Pooled"){
    res<-analysisFunBayesBetaPars(dat=dfSim,c=dfResPrimBayes$country[i],r=dfResPrimBayes$region[i],s=dfResPrimBayes$setting[i],doPlot=TRUE,plotFile=paste(sep="",outPrefix,"S4A_mcmcPars_",dfResPrimBayes$country[i],"_",dfResPrimBayes$region[i],"_",gsub(pattern=" / ",replacement="_",dfResPrimBayes$setting[i]),"_",gsub(pattern="-",replacement="",Sys.Date()),".pdf"),nChains=nChains,nAdapt=nAdapt,nBurn=nBurn,nIter=nIter,jagsFile=jagsFileW4ss,parsVect=parsVectW4ssNoLR)
    
    # derive positive and negative likelihood ratios
    res$mcmcObj<-getLRpLRm(res$mcmcObj)
    
    # format the summary
    mcmcSumDf<-MCMCsummary(res$mcmcObj)
    mcmcSumDf$mean<-format(nsmall=3,round(digits=3,mcmcSumDf$mean))
    mcmcSumDf$sd<-format(nsmall=3,round(digits=3,mcmcSumDf$sd))
    mcmcSumDf$`2.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`2.5%`))
    mcmcSumDf$`50%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`50%`))
    mcmcSumDf$`97.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`97.5%`))
    mcmcSumDfPrim<-mcmcSumDf[parsVectPrim,]
    mcmcSumDf<-mcmcSumDf[parsVectW4ss,]

    
    # print the results
    if(printResults){
      mcmcSumDfPrim %>%
        kable(caption = paste(sep="","Summary of the MCMC parameter estimation (individual tests only) for country ",dfResPrimBayes$country[i],", region ",dfResPrimBayes$region[i],", setting ",dfResPrimBayes$setting[i]," (PCF/ICF set of diagnostic algorithms).\nMetrics shown are the posterior mean, standard deviation, 2.5th, 50th, 97.5th quantiles, the Gelman-Rubin potential scale reduction factor (Rhat) and the effective sample size (n.eff).")) %>%
        kable_styling(full_width=FALSE)
    }
    
    # add the summary to the result object
    res[["mcmcSummary"]]<-mcmcSumDf
    
    allRes[[paste(sep="_",dfResPrimBayes$country[i],dfResPrimBayes$region[i],dfResPrimBayes$setting[i])]]<-res
    
    values<-numeric(0)
    for(j in 1:length(parsVectW4ss)){
      parTmp<-unlist(strsplit(split="\\.",parsVectW4ss[j]))
      parTmp.name<-tolower(paste(sep=".",parTmp[1],parTmp[3]))
      parTmp.par<-parTmp[2]
      if(!(parTmp.name %in% tolower(res$betaPars$par))){cat(paste(sep="","Parameter ",parTmp.name," not found in MCMC parameter list.\n"))}
      values<-c(values,res$betaPars[tolower(res$betaPars$par)==parTmp.name,parTmp.par])
    }
    
  }else if(dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$entryToStudy[i]=="W4SS" & dfResPrimBayes$country[i]!="Pooled"){
    res<-analysisFunBayesBetaPars(dat=dfSim,c=dfResPrimBayes$country[i],r=dfResPrimBayes$region[i],s=dfResPrimBayes$setting[i],doPlot=TRUE,plotFile=paste(sep="",outPrefix,"S4A_mcmcPars_",dfResPrimBayes$country[i],"_",dfResPrimBayes$region[i],"_",gsub(pattern=" / ",replacement="_",dfResPrimBayes$setting[i]),"_",gsub(pattern="-",replacement="",Sys.Date()),".pdf"),nChains=nChains,nAdapt=nAdapt,nBurn=nBurn,nIter=nIter,jagsFile=jagsFileW4ss,parsVect=parsVectW4ssNoLR)
    
    # derive positive and negative likelihood ratios
    res$mcmcObj<-getLRpLRm(res$mcmcObj)
    
    # format the summary
    mcmcSumDf<-MCMCsummary(res$mcmcObj)
    parsVectAcfNoLR<-parsVectAcf
    parsVectAcf<-unique(c(parsVectAcf,rownames(mcmcSumDf)))
    mcmcSumDf$mean<-format(nsmall=3,round(digits=3,mcmcSumDf$mean))
    mcmcSumDf$sd<-format(nsmall=3,round(digits=3,mcmcSumDf$sd))
    mcmcSumDf$`2.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`2.5%`))
    mcmcSumDf$`50%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`50%`))
    mcmcSumDf$`97.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`97.5%`))
    mcmcSumDfPrim<-mcmcSumDf[parsVectPrim,]
    mcmcSumDf<-mcmcSumDf[parsVectAcf,]

    # print the results
    if(printResults){
      mcmcSumDfPrim %>%
        kable(caption = paste(sep="","Summary of the MCMC parameter estimation (individual tests only) for country ",dfResPrimBayes$country[i],", region ",dfResPrimBayes$region[i],", setting ",dfResPrimBayes$setting[i]," (ACF set of diagnostic algorithms).\nMetrics shown are the posterior mean, standard deviation, 2.5th, 50th, 97.5th quantiles, the Gelman-Rubin potential scale reduction factor (Rhat) and the effective sample size (n.eff).")) %>%
        kable_styling(full_width=FALSE)
    }
    
    # add the summary to the result object
    res[["mcmcSummary"]]<-mcmcSumDf
    
    allRes[[paste(sep="_",dfResPrimBayes$country[i],dfResPrimBayes$region[i],dfResPrimBayes$setting[i])]]<-res
    
    values<-numeric(0)
    for(j in 1:length(parsListAcf)){
      parTmp<-unlist(strsplit(split="\\.",parsListAcf[j]))
      parTmp.name<-tolower(paste(sep=".",parTmp[1],parTmp[3]))
      parTmp.par<-parTmp[2]
      if(!(parTmp.name %in% tolower(res$betaPars$par))){cat(paste(sep="","Parameter ",parTmp.name," not found in MCMC parameter list.\n"))}
      values<-c(values,res$betaPars[tolower(res$betaPars$par)==parTmp.name,parTmp.par])
    }
    
  }else if(dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$entryToStudy[i]=="CXR" & dfResPrimBayes$country[i]!="Pooled"){
    res<-analysisFunBayesBetaPars(dat=dfSim,c=dfResPrimBayes$country[i],r=dfResPrimBayes$region[i],s=dfResPrimBayes$setting[i],doPlot=TRUE,plotFile=paste(sep="",outPrefix,"S4A_mcmcPars_",dfResPrimBayes$country[i],"_",dfResPrimBayes$region[i],"_",gsub(pattern=" / ",replacement="_",dfResPrimBayes$setting[i]),"_",gsub(pattern="-",replacement="",Sys.Date()),".pdf"),nChains=nChains,nAdapt=nAdapt,nBurn=nBurn,nIter=nIter,jagsFile=jagsFileCxr,parsVect=parsVectCxrNoLR)
    
    # derive positive and negative likelihood ratios
    res$mcmcObj<-getLRpLRm(res$mcmcObj)
    
    # format the summary
    mcmcSumDf<-MCMCsummary(res$mcmcObj)
    parsVectAcfNoLR<-parsVectAcf
    parsVectAcf<-unique(c(parsVectAcf,rownames(mcmcSumDf)))
    mcmcSumDf$mean<-format(nsmall=3,round(digits=3,mcmcSumDf$mean))
    mcmcSumDf$sd<-format(nsmall=3,round(digits=3,mcmcSumDf$sd))
    mcmcSumDf$`2.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`2.5%`))
    mcmcSumDf$`50%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`50%`))
    mcmcSumDf$`97.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`97.5%`))
    mcmcSumDfPrim<-mcmcSumDf[parsVectPrim,]
    mcmcSumDf<-mcmcSumDf[parsVectAcf,]

    # print the results
    if(printResults){
      mcmcSumDfPrim %>%
        kable(caption = paste(sep="","Summary of the MCMC parameter estimation (individual tests only) for country ",dfResPrimBayes$country[i],", region ",dfResPrimBayes$region[i],", setting ",dfResPrimBayes$setting[i]," (ACF set of diagnostic algorithms).\nMetrics shown are the posterior mean, standard deviation, 2.5th, 50th, 97.5th quantiles, the Gelman-Rubin potential scale reduction factor (Rhat) and the effective sample size (n.eff).")) %>%
        kable_styling(full_width=FALSE)
    }
    
    # add the summary to the result object
    res[["mcmcSummary"]]<-mcmcSumDf
    
    allRes[[paste(sep="_",dfResPrimBayes$country[i],dfResPrimBayes$region[i],dfResPrimBayes$setting[i])]]<-res
    
    values<-numeric(0)
    for(j in 1:length(parsListAcf)){
      parTmp<-unlist(strsplit(split="\\.",parsListAcf[j]))
      parTmp.name<-tolower(paste(sep=".",parTmp[1],parTmp[3]))
      parTmp.par<-parTmp[2]
      if(!(parTmp.name %in% tolower(res$betaPars$par))){cat(paste(sep="","Parameter ",parTmp.name," not found in MCMC parameter list.\n"))}
      values<-c(values,res$betaPars[tolower(res$betaPars$par)==parTmp.name,parTmp.par])
    }
    
  }else if(dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$entryToStudy[i]=="W4SS | CXR" & dfResPrimBayes$country[i]!="Pooled"){
    res<-analysisFunBayesBetaPars(dat=dfSim,c=dfResPrimBayes$country[i],r=dfResPrimBayes$region[i],s=dfResPrimBayes$setting[i],doPlot=TRUE,plotFile=paste(sep="",outPrefix,"S4A_mcmcPars_",dfResPrimBayes$country[i],"_",dfResPrimBayes$region[i],"_",gsub(pattern=" / ",replacement="_",dfResPrimBayes$setting[i]),"_",gsub(pattern="-",replacement="",Sys.Date()),".pdf"),nChains=nChains,nAdapt=nAdapt,nBurn=nBurn,nIter=nIter,jagsFile=jagsFileCxr,parsVect=parsVectCxrNoLR)
    
    # derive positive and negative likelihood ratios
    res$mcmcObj<-getLRpLRm(res$mcmcObj)
    
    # format the summary
    mcmcSumDf<-MCMCsummary(res$mcmcObj)
    parsVectAcfNoLR<-parsVectAcf
    parsVectAcf<-unique(c(parsVectAcf,rownames(mcmcSumDf)))
    mcmcSumDf$mean<-format(nsmall=3,round(digits=3,mcmcSumDf$mean))
    mcmcSumDf$sd<-format(nsmall=3,round(digits=3,mcmcSumDf$sd))
    mcmcSumDf$`2.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`2.5%`))
    mcmcSumDf$`50%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`50%`))
    mcmcSumDf$`97.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`97.5%`))
    mcmcSumDfPrim<-mcmcSumDf[parsVectPrim,]
    mcmcSumDf<-mcmcSumDf[parsVectAcf,]

    # print the results
    if(printResults){
      mcmcSumDfPrim %>%
        kable(caption = paste(sep="","Summary of the MCMC parameter estimation (individual tests only) for country ",dfResPrimBayes$country[i],", region ",dfResPrimBayes$region[i],", setting ",dfResPrimBayes$setting[i]," (ACF set of diagnostic algorithms).\nMetrics shown are the posterior mean, standard deviation, 2.5th, 50th, 97.5th quantiles, the Gelman-Rubin potential scale reduction factor (Rhat) and the effective sample size (n.eff).")) %>%
        kable_styling(full_width=FALSE)
    }
    
    # add the summary to the result object
    res[["mcmcSummary"]]<-mcmcSumDf
    
    allRes[[paste(sep="_",dfResPrimBayes$country[i],dfResPrimBayes$region[i],dfResPrimBayes$setting[i])]]<-res
    
    values<-numeric(0)
    for(j in 1:length(parsListAcf)){
      parTmp<-unlist(strsplit(split="\\.",parsListAcf[j]))
      parTmp.name<-tolower(paste(sep=".",parTmp[1],parTmp[3]))
      parTmp.par<-parTmp[2]
      if(!(parTmp.name %in% tolower(res$betaPars$par))){cat(paste(sep="","Parameter ",parTmp.name," not found in MCMC parameter list.\n"))}
      values<-c(values,res$betaPars[tolower(res$betaPars$par)==parTmp.name,parTmp.par])
    }
    
  }else if(dfResPrimBayes$setting[i]=="Children" & dfResPrimBayes$country[i]!="Pooled"){
    cat("Children processing not yet implemented.\n")
    values<-NA
  }else if(dfResPrimBayes$country[i]!="Pooled"){
    stop("Invalid setting parameter.")
  }
  
  if(!checkW4ssFacility & dfResPrimBayes$setting[i] %in% c("PHC","District") & dfResPrimBayes$country[i]!="Pooled"){
    dfResW4ssFacility<-values
    checkW4ssFacility<-TRUE
  }else if(checkW4ssFacility & dfResPrimBayes$setting[i] %in% c("PHC","District") & dfResPrimBayes$country[i]!="Pooled"){
    dfResW4ssFacility<-rbind(dfResW4ssFacility,values)
  }else if(!checkW4ssAcf & dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$country[i]!="Pooled" & dfResPrimBayes$entryToStudy[i]=="W4SS"){
    dfResW4ssAcf<-values
    checkW4ssAcf<-TRUE
  }else if(checkW4ssAcf & dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$country[i]!="Pooled" & dfResPrimBayes$entryToStudy[i]=="W4SS"){
    dfResW4ssAcf<-rbind(dfResW4ssAcf,values)
  }else if(!checkW4ssCxrAcf & dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$country[i]!="Pooled" & dfResPrimBayes$entryToStudy[i]=="W4SS | CXR"){
    dfResW4ssCxrAcf<-values
    checkW4ssCxrAcf<-TRUE
  }else if(checkW4ssCxrAcf & dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$country[i]!="Pooled" & dfResPrimBayes$entryToStudy[i]=="W4SS | CXR"){
    dfResW4ssCxrAcf<-rbind(dfResW4ssCxrAcf,values)
  }else if(!checkCxrAcf & dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$country[i]!="Pooled" & dfResPrimBayes$entryToStudy[i]=="W4SS | CXR"){
    dfResCxrAcf<-values
    checkCxrAcf<-TRUE
  }else if(checkCxrAcf & dfResPrimBayes$setting[i] %in% c("IDP / refugees","Informal settlements","Nomads") & dfResPrimBayes$country[i]!="Pooled" & dfResPrimBayes$entryToStudy[i]=="W4SS | CXR"){
    dfResCxrAcf<-rbind(dfResCxrAcf,values)
  }
}


# Pooled PHC
res<-analysisFunBayesBetaParsPool(dat=dfSim,set="PHC",doPlot=TRUE,plotFile=paste(sep="",outPrefix,"S4A_mcmcPars_Pooled_PHC","_",gsub(pattern="-",replacement="",Sys.Date()),".pdf"),nChains=nChains,nAdapt=nAdapt,nBurn=nBurn,nIter=nIter,jagsFile=jagsFileW4ssPooled,parsVect=parsVectW4ss)
    
## derive positive and negative likelihood ratios
res$mcmcObj<-getLRpLRm(res$mcmcObj)

## format the summary
mcmcSumDf<-MCMCsummary(res$mcmcObj)
mcmcSumDf$mean<-format(nsmall=3,round(digits=3,mcmcSumDf$mean))
mcmcSumDf$sd<-format(nsmall=3,round(digits=3,mcmcSumDf$sd))
mcmcSumDf$`2.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`2.5%`))
mcmcSumDf$`50%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`50%`))
mcmcSumDf$`97.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`97.5%`))
mcmcSumDfPrim<-mcmcSumDf[parsVectPrim,]
mcmcSumDf<-mcmcSumDf[parsVectW4ss,]

# add the summary to the result object
res[["mcmcSummary"]]<-mcmcSumDf

allRes[["Pooled__PHC"]]<-res

values<-numeric(0)
for(j in 1:length(parsVectW4ss)){
  parTmp<-unlist(strsplit(split="\\.",parsVectW4ss[j]))
  parTmp.name<-tolower(paste(sep=".",parTmp[1],parTmp[3]))
  parTmp.par<-parTmp[2]
  if(!(parTmp.name %in% tolower(res$betaPars$par))){cat(paste(sep="","Parameter ",parTmp.name," not found in MCMC parameter list.\n"))}
  values<-c(values,res$betaPars[tolower(res$betaPars$par)==parTmp.name,parTmp.par])
}

dfRes<-rbind(dfResW4ssFacility,values)

# Pooled district
res<-analysisFunBayesBetaParsPool(dat=dfSim,set="District",doPlot=TRUE,plotFile=paste(sep="",outPrefix,"S4A_mcmcPars_Pooled_District","_",gsub(pattern="-",replacement="",Sys.Date()),".pdf"),nChains=nChains,nAdapt=nAdapt,nBurn=nBurn,nIter=nIter,jagsFile=jagsFileW4ssPooled,parsVect=parsVectW4ss)
    
## derive positive and negative likelihood ratios
res$mcmcObj<-getLRpLRm(res$mcmcObj)

## format the summary
mcmcSumDf<-MCMCsummary(res$mcmcObj)
mcmcSumDf$mean<-format(nsmall=3,round(digits=3,mcmcSumDf$mean))
mcmcSumDf$sd<-format(nsmall=3,round(digits=3,mcmcSumDf$sd))
mcmcSumDf$`2.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`2.5%`))
mcmcSumDf$`50%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`50%`))
mcmcSumDf$`97.5%`<-format(nsmall=3,round(digits=3,mcmcSumDf$`97.5%`))
mcmcSumDfPrim<-mcmcSumDf[parsVectPrim,]
mcmcSumDf<-mcmcSumDf[parsVectW4ss,]

# add the summary to the result object
res[["mcmcSummary"]]<-mcmcSumDf

allRes[["Pooled__District"]]<-res

values<-numeric(0)
for(j in 1:length(parsVectW4ss)){
  parTmp<-unlist(strsplit(split="\\.",parsVectW4ss[j]))
  parTmp.name<-tolower(paste(sep=".",parTmp[1],parTmp[3]))
  parTmp.par<-parTmp[2]
  if(!(parTmp.name %in% tolower(res$betaPars$par))){cat(paste(sep="","Parameter ",parTmp.name," not found in MCMC parameter list.\n"))}
  values<-c(values,res$betaPars[tolower(res$betaPars$par)==parTmp.name,parTmp.par])
}

dfResW4ssFacility<-rbind(dfResW4ssFacility,values)

if(!is.null(dim(dfResW4ssFacility))){
  dfResW4ssFacility<-cbind(dfResPrimBayes %>% dplyr::filter(setting %in% c("PHC","District")),dfResW4ssFacility)
  colnames(dfResW4ssFacility)<-c(colnames(dfResPrimBayes),parsVectW4ss)
}else{
  dfResW4ssFacility<-as.character(c(dfResPrimBayes %>% dplyr::filter(setting %in% c("PHC","District")),dfResW4ssFacility))
  names(dfResW4ssFacility)<-c(colnames(dfResPrimBayes),parsVectW4ss)
  dfResW4ssFacility<-t(as.data.frame(dfResW4ssFacility))
}

# if(!is.null(dim(dfResAcf))){
#   dfResAcf<-cbind(dfResPrimBayes %>% dplyr::filter(setting %in% c("IDP / refugees","Informal settlements","Nomads")),dfResAcf)
#   colnames(dfResAcf)<-c(colnames(dfResPrimBayes),parsListAcf)
# }else{
#   dfResAcf<-as.character(c(dfResPrimBayes %>% dplyr::filter(setting %in% c("IDP / refugees","Informal settlements","Nomads")),dfResAcf))
#   names(dfResAcf)<-c(colnames(dfResPrimBayes),parsListAcf)
#   dfResAcf<-t(as.data.frame(dfResAcf))
# }

write.csv(dfResW4ssFacility,row.names=F,file=paste(sep="","mockData_AnalysisResults_PCFICF_",gsub(pattern="-",replacement="",Sys.Date()),".csv"))
#write.csv(dfResAcf,row.names=F,file=paste(sep="","mockData_AnalysisResults_ACF_",gsub(pattern="-",replacement="",Sys.Date()),".csv"))
save(allRes,file=paste(sep="","mockData_AnalysisResults_",gsub(pattern="-",replacement="",Sys.Date()),".RData"))
```

<!--
Below in @tbl-primAnalBayesOneCountryPcf we present how we will present results for the overall PCF/ICF evaluations - taking the example of Malawi PHC.

```{r}
#| eval: false
#| label: tbl-primAnalBayesOneCountryPcf
#| tbl-cap: Performance metric estimates of the evaluated tests in a PCF/ICF setting for an example country (Malawi).

res<-allRes[["Malawi__PHC"]]

algoNames<-c("ssm","xpert","lam","cxrlam","crplam","cxrcrplam","cxr.xpert","cxr.lam","crp.xpert","cxr.crplam","crp.lam","lam.xpert","poolxpert.xpert","cxrcrp.xpert","cxrcrp.lam","cxrlam.xpert","crplam.xpert","cxrcrplam.xpert","cxr.crp.xpert","cxr.crp.lam","cxr.lam.xpert","cxr.poolxpert.xpert","crp.poolxpert.xpert","lam.poolxpert.xpert","cxr.crplam.xpert","cxrcrp.poolxpert.xpert","cxrlam.poolxpert.xpert","crplam.poolxpert.xpert","cxrcrplam.poolxpert.xpert","cxr.crp.poolxpert.xpert","cxr.lam.poolxpert.xpert","cxr.crplam.poolxpert.xpert")

algoLabels<-c("SSM","Xpert Ultra","LAM","CXR+LAM","CRP+LAM","CXR+CRP+LAM","CXR -> Xpert Ultra","CXR -> LAM","CRP -> Xpert Ultra","CXR -> CRP+LAM","CRP -> LAM","LAM -> Xpert Ultra","Pooled Xpert Ultra -> Xpert Ultra","CXR+CRP -> Xpert Ultra","CXR+CRP -> LAM","CXR+LAM -> Xpert Ultra","CRP+LAM -> Xpert Ultra","CXR+CRP+LAM -> Xpert Ultra","CXR -> CRP -> Xpert Ultra","CXR -> CRP -> LAM","CXR -> LAM -> Xpert Ultra","CXR -> Pooled Xpert Ultra -> Xpert Ultra","CRP -> Pooled Xpert Ultra -> Xpert Ultra","LAM -> Pooled Xpert Ultra -> Xpert Ultra","CXR -> CRP+LAM -> Xpert Ultra","CXR+CRP -> Pooled Xpert Ultra -> Xpert Ultra","CXR+LAM -> Pooled Xpert Ultra -> Xpert Ultra","CRP+LAM -> Pooled Xpert Ultra -> Xpert Ultra","CXR+CRP+LAM -> Pooled Xpert Ultra -> Xpert Ultra","CXR -> CRP -> Pooled Xpert Ultra -> Xpert Ultra","CXR -> LAM -> Pooled Xpert Ultra -> Xpert Ultra","CXR -> CRP+LAM -> Pooled Xpert Ultra -> Xpert Ultra")

dfAlgos<-data.frame(
  algo=algoNames,
  sens=NA,
  spec=NA,
  ppv=NA,
  npv=NA,
  dyt=NA,
  dyd=NA,
  lrp=NA,
  lrm=NA
)

for(j in 1:nrow(dfAlgos)){
  par<-dfAlgos$algo[j]
  
  # sensitivity
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","pse.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","pse.",par),c("2.5%","97.5%")])),")")
  dfAlgos$sens[j]<-paste(sep=" ",pointEstimate,ci)
  
  # specificity
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","psp.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","psp.",par),c("2.5%","97.5%")])),")")
  dfAlgos$spec[j]<-paste(sep=" ",pointEstimate,ci)
  
  # PPV
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","ppv.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","ppv.",par),c("2.5%","97.5%")])),")")
  dfAlgos$ppv[j]<-paste(sep=" ",pointEstimate,ci)
  
  # NPV
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","npv.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","npv.",par),c("2.5%","97.5%")])),")")
  dfAlgos$npv[j]<-paste(sep=" ",pointEstimate,ci)
  
  # DYT
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","dyt.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","dyt.",par),c("2.5%","97.5%")])),")")
  dfAlgos$dyt[j]<-paste(sep=" ",pointEstimate,ci)
  
  # # DYD
  # pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","dyd.",par),"50%"])
  # ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","dyd.",par),c("2.5%","97.5%")])),")")
  # dfAlgos$dyd[j]<-paste(sep=" ",pointEstimate,ci)
  
  # LR+
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","lrp.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","lrp.",par),c("2.5%","97.5%")])),")")
  dfAlgos$lrp[j]<-paste(sep=" ",pointEstimate,ci)
  
  # LR-
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","lrm.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","lrm.",par),c("2.5%","97.5%")])),")")
  dfAlgos$lrm[j]<-paste(sep=" ",pointEstimate,ci)
}

colnames(dfAlgos)<-case_when(
  colnames(dfAlgos)=="algo"~"Algorithm code",
  colnames(dfAlgos)=="label"~"Algorithm",
  colnames(dfAlgos)=="sens"~"Sensitivity",
  colnames(dfAlgos)=="spec"~"Specificity",
  colnames(dfAlgos)=="ppv"~"PPV",
  colnames(dfAlgos)=="npv"~"NPV",
  colnames(dfAlgos)=="dyt"~"DYT",
  colnames(dfAlgos)=="dyd"~"DYD",
  colnames(dfAlgos)=="lrp"~"Positive LR",
  colnames(dfAlgos)=="lrm"~"Negative LR"
)

dfAlgos %>%
  dplyr::select(!contains("code") & !contains("DYD")) %>%
  kable(caption="PCF/ICF algorithm evaluation for Cameroon PHC.") %>%
  kable_styling(full_width = FALSE)
```

```{r}
#| eval: false
dfAlgos %>%
  filter(as.numeric(res$mcmcSummary[paste(sep=".","pse",dfAlgos$`Algorithm code`),"50%"])>0.75 & as.numeric(res$mcmcSummary[paste(sep=".","psp",dfAlgos$`Algorithm code`),"50%"])>0.95) %>%
  dplyr::select(!contains("code") & !contains("DYD")) %>%
  kable(caption="PCF/ICF algorithm evaluation for Malawi PHC (with sensitivity>0.75 and specificity>0.95).") %>%
  kable_styling(full_width = FALSE)
```

```{r}
#| eval: false
dfAlgos %>%
  filter(as.numeric(res$mcmcSummary[paste(sep=".","ppv",dfAlgos$`Algorithm code`),"50%"])>0.92 & as.numeric(res$mcmcSummary[paste(sep=".","npv",dfAlgos$`Algorithm code`),"50%"])>0.92) %>%
  dplyr::select(!contains("code") & !contains("DYD")) %>%
  kable(caption="PCF/ICF algorithm evaluation for Malawi PHC (with PPV>0.92 and NPV>0.92).") %>%
  kable_styling(full_width = FALSE)
```

-->

#### PCF / ICF district level

<!--
These PCF/ICF results (which will be for the exact same algorithms) will be presented in the same way as the results for the lowest level of care setting.
-->

#### ACF W4SS entry

<!--
Below in @tbl-primAnalBayesOneCountryAcfW4ss we present how we will present results for the overall ACF evaluations - taking the example of Cameroon informal settlements.

```{r}
#| eval: false
#| label: tbl-primAnalBayesOneCountryAcf
#| tbl-cap: Performance metric estimates of the evaluated tests in an ACF setting for an example country (Malawi).

res<-allRes[["Cameroon__Informal settlements ACF"]]

algoNames<-c("w4ss.ssm","w4ss.xpert","w4ss.lam","w4ss.cxrlam","w4ss.crplam","cxr.xpert","cxr.lam","cxr.crplam","w4sscxr.xpert","w4sscxr.lam","w4sscxr.crplam","w4ss.cxr.xpert","w4ss.cxr.lam","w4ss.crp.xpert","w4ss.crp.lam","w4ss.lam.xpert","w4ss.poolxpert.xpert","w4ss.cxrcrp.xpert","w4ss.cxrcrp.lam","w4ss.cxrlam.xpert","w4ss.crplam.xpert","w4ss.cxrcrplam.xpert","cxr.crp.xpert","cxr.crp.lam","cxr.lam.xpert","cxr.crplam.xpert","cxr.poolxpert.xpert","w4sscxr.crp.lam","w4sscxr.crp.lam","w4sscxr.lam.xpert","w4sscxr.crplam.xpert","w4sscxr.poolxpert.xpert", "w4ss.cxr.poolxpert.xpert","w4ss.crp.poolxpert.xpert","w4ss.lam.poolxpert.xpert","w4ss.cxrcrp.poolxpert.xpert","w4ss.cxrlam.poolxpert.xpert","w4ss.crplam.poolxpert.xpert","w4ss.cxrcrplam.poolxpert.xpert","cxr.crp.poolxpert.xpert","cxr.lam.poolxpert.xpert","cxr.crplam.poolxpert.xpert","w4sscxr.crp.poolxpert.xpert","w4sscxr.lam.poolxpert.xpert","w4sscxr.crplam.poolxpert.xpert")

algoLabels<-c("W4SS -> SSM","W4SS -> Xpert Ultra","W4SS -> LAM","W4SS -> CXR+LAM","W4SS -> CRP+LAM","CXR -> Xpert Ultra","CXR -> LAM","CXR -> CRP+LAM","W4SS+CXR -> Xpert Ultra","W4SS+CXR -> LAM","W4SS+CXR -> CRP+LAM","W4SS -> CXR -> Xpert Ultra","W4SS -> CXR -> LAM","W4SS -> CRP -> Xpert Ultra","W4SS -> CRP -> LAM","W4SS -> LAM -> Xpert Ultra","W4SS -> Pooled Xpert Ultra -> Xpert Ultra","W4SS -> CXR+CRP -> Xpert Ultra","W4SS -> CXR+CRP -> LAM","W4SS -> CXR+LAM -> Xpert Ultra","W4SS -> CRP+LAM -> Xpert Ultra","W4SS -> CXR+CRP+LAM -> Xpert Ultra","CXR -> CRP -> Xpert Ultra","CXR -> CRP -> LAM","CXR -> LAM -> Xpert Ultra","CXR -> CRP+LAM -> Xpert Ultra","CXR -> Pooled Xpert Ultra -> Xpert Ultra","W4SS+CXR -> CRP -> LAM","W4SS+CXR -> CRP -> LAM","W4SS+CXR -> LAM -> Xpert Ultra","W4SS+CXR -> CRP+LAM -> Xpert Ultra","W4SS+CXR -> Pooled Xpert Ultra -> Xpert Ultra","W4SS -> CXR -> Pooled Xpert Ultra -> Xpert Ultra","W4SS -> CRP -> Pooled Xpert Ultra -> Xpert Ultra","W4SS -> LAM -> Pooled Xpert Ultra -> Xpert Ultra","W4SS -> CXR+CRP -> Pooled Xpert Ultra -> Xpert Ultra","W4SS -> CXR+LAM -> Pooled Xpert Ultra -> Xpert Ultra","W4SS -> CRP+LAM -> Pooled Xpert Ultra -> Xpert Ultra","W4SS -> CXR+CRP+LAM -> Pooled Xpert Ultra -> Xpert Ultra","CXR -> CRP -> Pooled Xpert Ultra -> Xpert Ultra","CXR -> LAM -> Pooled Xpert Ultra -> Xpert Ultra","CXR -> CRP+LAM -> Pooled Xpert Ultra -> Xpert Ultra","W4SS+CXR -> CRP -> Pooled Xpert Ultra -> Xpert Ultra","W4SS+CXR -> LAM -> Pooled Xpert Ultra -> Xpert Ultra","W4SS+CXR -> CRP+LAM -> Pooled Xpert Ultra -> Xpert Ultra")

dfAlgos<-data.frame(
  algo=algoNames,
  label=algoLabels,
  sens=NA,
  spec=NA,
  ppv=NA,
  npv=NA,
  dyt=NA,
  dyd=NA,
  lrp=NA,
  lrm=NA
)


for(j in 1:nrow(dfAlgos)){
  par<-dfAlgos$algo[j]
  
  # sensitivity
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","pse.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","pse.",par),c("2.5%","97.5%")])),")")
  dfAlgos$sens[j]<-paste(sep=" ",pointEstimate,ci)
  
  # specificity
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","psp.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","psp.",par),c("2.5%","97.5%")])),")")
  dfAlgos$spec[j]<-paste(sep=" ",pointEstimate,ci)
  
  # PPV
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","ppv.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","ppv.",par),c("2.5%","97.5%")])),")")
  dfAlgos$ppv[j]<-paste(sep=" ",pointEstimate,ci)
  
  # NPV
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","npv.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","npv.",par),c("2.5%","97.5%")])),")")
  dfAlgos$npv[j]<-paste(sep=" ",pointEstimate,ci)
  
  # DYT
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","dyt.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","dyt.",par),c("2.5%","97.5%")])),")")
  dfAlgos$dyt[j]<-paste(sep=" ",pointEstimate,ci)
  
  # # DYD
  # pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","dyd.",par),"50%"])
  # ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","dyd.",par),c("2.5%","97.5%")])),")")
  # dfAlgos$dyd[j]<-paste(sep=" ",pointEstimate,ci)
  
  # LR+
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","lrp.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","lrp.",par),c("2.5%","97.5%")])),")")
  dfAlgos$lrp[j]<-paste(sep=" ",pointEstimate,ci)
  
  # LR-
  pointEstimate<-gsub(pattern=" ",replacement="",res$mcmcSummary[paste(sep="","lrm.",par),"50%"])
  ci<-paste(sep="","(",gsub(pattern=" ",replacement="",paste(collapse=",",res$mcmcSummary[paste(sep="","lrm.",par),c("2.5%","97.5%")])),")")
  dfAlgos$lrm[j]<-paste(sep=" ",pointEstimate,ci)
}

colnames(dfAlgos)<-case_when(
  colnames(dfAlgos)=="algo"~"Algorithm code",
  colnames(dfAlgos)=="label"~"Algorithm",
  colnames(dfAlgos)=="sens"~"Sensitivity",
  colnames(dfAlgos)=="spec"~"Specificity",
  colnames(dfAlgos)=="ppv"~"PPV",
  colnames(dfAlgos)=="npv"~"NPV",
  colnames(dfAlgos)=="dyt"~"DYT",
  colnames(dfAlgos)=="dyd"~"DYD",
  colnames(dfAlgos)=="lrp"~"Positive LR",
  colnames(dfAlgos)=="lrm"~"Negative LR"
)

dfAlgos %>%
  dplyr::select(!(contains("code") | contains("DYD"))) %>%
  kable(caption="ACF algorithm evaluation for Cameroon informal settlements.") %>%
  kable_styling(full_width = FALSE)
```

-->

#### ACF CXR-CAD entry

<!-- 
Below in @tbl-primAnalBayesOneCountryAcfCadCxr we present how we will present results for the overall ACF evaluations - taking the example of Cameroon informal settlements.
-->

#### ACF (W4SS or CXR-CAD) entry

<!--
These ACF results (which will be for the exact same algorithms) will be presented in the same way as the results for the ACF CXR-CAD entry.
-->

## Link-up with the health economic analysis

### Beta distributions for parameters

All (except for the likelihood ratios) of the performance metrics and parameters associated with intermediate steps in the different algorithms that we estimate are probability parameters and they will directly inform the decision tree model used in the planned health economic analysis. To allow propagating the uncertainty of each of the above estimates, we will need to input the entire posterior distribution for each parameter into the health economic modelling.

The easiest way, for each parameter, to do this is to use quantile matching to identify the best fitting beta distribution for each empirical distribution. We will then use the estimated beta distribution parameters to feed into the health economic modelling.

<!--
To illustrate this computation, @tbl-primAnalBayesBeta shows the matrix of the estimated $a,b$ values, obtained for the posterior distribution for the performance estimates of the individual tests.

```{r}
#| eval: false

dfResPrimBetaBayes<-data.frame(
  country=c(rep("Cameroon",4),rep("Nigeria",4),rep("Kenya",2),rep("Bangladesh",3),rep("Brazil",2),rep("Vietnam",3),rep("Malawi",1)),
  region=c(rep("",4+4+2+3),"Aracaju","Maceio",rep("",3+1)),
  setting=c("PHC","District","Informal settlements ACF","Children","PHC","District","Nomads","IDP / refugees","PHC","District","PHC","District","Informal settlements ACF","PHC","PHC","PHC","Informal settlements ACF","Children","PHC"),
  prevalenceTB_a=NA,
  prevalenceTB_b=NA,
  posPropHiv_a=NA,
  posPropHiv_b=NA,
  sensHiv_a=NA,
  sensHiv_b=NA,
  specHiv_a=NA,
  specHiv_b=NA,
  ppvHiv_a=NA,
  ppvHiv_b=NA,
  npvHiv_a=NA,
  npvHiv_b=NA,
  posPropCrp_a=NA,
  posPropCrp_b=NA,
  sensCrp_a=NA,
  sensCrp_b=NA,
  specCrp_a=NA,
  specCrp_b=NA,
  ppvCrp_a=NA,
  ppvCrp_b=NA,
  npvCrp_a=NA,
  npvCrp_b=NA,
  posPropLam_a=NA,
  posPropLam_b=NA,
  sensLam_a=NA,
  sensLam_b=NA,
  specLam_a=NA,
  specLam_b=NA,
  ppvLam_a=NA,
  ppvLam_b=NA,
  npvLam_a=NA,
  npvLam_b=NA,
  posPropPoolxpert_a=NA,
  posPropPoolxpert_b=NA,
  sensPoolxpert_a=NA,
  sensPoolxpert_b=NA,
  specPoolxpert_a=NA,
  specPoolxpert_b=NA,
  ppvPoolxpert_a=NA,
  ppvPoolxpert_b=NA,
  npvPoolxpert_a=NA,
  npvPoolxpert_b=NA,
  posPropCxr_a=NA,
  posPropCxr_b=NA,
  sensCxr_a=NA,
  sensCxr_b=NA,
  specCxr_a=NA,
  specCxr_b=NA,
  ppvCxr_a=NA,
  ppvCxr_b=NA,
  npvCxr_a=NA,
  npvCxr_b=NA
)

for(i in 1:nrow(dfResPrimBetaBayes)){
  #dfResPrimBetaBayes[i,-(1:3)]<-analysisFunBayesBetaPars(dat=dfSim,c=dfResPrimBetaBayes$country[i],r=dfResPrimBetaBayes$region[i],s=dfResPrimBetaBayes$setting[i])
}
```

```{r}
#| eval: false
#| label: tbl-primAnalBayesBeta
#| tbl-cap: "Posterior beta distribution parameters for the unconditional probability parameters."

dfResPrimBetaBayes %>%
  dplyr::select(!country) %>%
  kable(row.names=FALSE,col.names=c("Region","Setting",rep(c("a","b"),26)),digits=2) %>%
  kable_styling(full_width=FALSE) %>%
  add_header_above(c(" "=2,"Prevalence of TB"=2,rep(c("Proportion of positive tests"=2,"Sensitivity"=2,"Specificity"=2,"PPV"=2,"NPV"=2),5))) %>%
  add_header_above(c(" "=4,"HIV"=10,"CRP"=10,"LAM"=10,"Pooled Xpert"=10,"Digital chest X-ray + CAD"=10)) %>%
  pack_rows("Cameroon",1,4) %>%
  pack_rows("Nigeria",5,8) %>%
  pack_rows("Kenya",9,10) %>%
  pack_rows("Bangladesh",11,13) %>%
  pack_rows("Brazil",14,15) %>%
  pack_rows("Vietnam",16,18) %>%
  pack_rows("Malawi",19,19)
```

-->


# List of figures

None in this version of the SAP. The final version will include mock-ups of the forest plots for the performance evaluations.

# List of tables

@tbl-abbr: List of abbreviations

@tbl-versionHistory: Version history of the SAP since v1.0.

@tbl-countries: Summary of study populations, countries, and procedures.

@tbl-diagnostics: Summary of diagnostic assays evaluated in this study.

@tbl-sampSizeCalc: Sample size calculation for adults.

@tbl-sampSizeTableByCountry: Sample size for each survey.

@tbl-dataSim: 10 random rows from the simulated data.

@tbl-dataSummary: Summary of the study data.

@tbl-perfSummary: Summary of the performance of individual tests compared against culture and Xpert Ultra.


The final version of the SAP will include mock-ups of table with the performance evaluation results.

<!--
@tbl-primAnalBayesOneCountryPcf: Performance metric estimates of the evaluated tests in a PCF/ICF setting for an example country (Malawi).

@tbl-primAnalBayesOneCountryAcfW4ss: Performance metric estimates of the evaluated tests in an ACF setting, with W4SS entry, for an example country (Malawi).

@tbl-primAnalBayesOneCountryAcfCxrCad: Performance metric estimates of the evaluated tests in an ACF setting, with CXR-CAD entry, for an example country (Malawi).

@tbl-primAnalBayesBeta: Posterior beta distribution parameters for the unconditional probability parameters.

@tbl-primAnalBayesAlgo: Results for an example screening algorithm. We only report the posterior beta distribution parameters for each estimated probability parameter.
-->

# References
